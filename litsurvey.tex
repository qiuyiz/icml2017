
\subsection{Related Work.}
If the activation functions are linear or
if some independence assumptions are made, Kawaguchi shows that the
only local minima are the global minima \cite{Kawaguchi16a}. Under the
spin-glass and other physical models, some have shown that the loss
landscape admits well-behaving local minima that occur usually when the
overall error is small
\cite{ChoromanskaHMAL14, DauphinPGCGB14}. When only training
error is considered, some have shown that a global minima can be
achieved if the neural network contains sufficiently many hidden nodes
\cite{SoudryC16}. Recently, Daniely has shown that SGD learns the conjugate kernel class \cite{daniely2017sgd}. Under simplifying assumptions, some results for learning ReLU's with gradient descent are given in \cite{tian2017analytical, brutzkus2017globally}. Our research is inspired by
\cite{valiant2014learning}, where the authors show that for polynomial 
target functions, gradient descent on neural networks
with one hidden layer converges to low error, given a large
number of hidden nodes, and under complex perturbations,
there are no robust local minima. Even more recently, similar results about the convergence of SGD for two-layer neural networks have been established for a polynomial activation function under a more complex loss function \cite{ge2017learning}. And in \cite{li2017convergence}, they study the same problem as ours with the RELU activation and where lower layer of the network is close to identity and the upper layer has weights all one. This corresponds to the case where each electron is close to a distinct proton -- under these assumptions they show that SGD learns the true network. 

Under worst case assumptions, there has been hardness results for even simple networks. A neural network with one hidden unit and sigmoidal activation can admit exponentially many local minima \cite{Auer}. Backprogration has been proven to fail in a simple network due to the abundance of bad local minima \cite{brady1989back}. Training a 3-node neural network with one hidden layer is { NP}-complete \cite{BlumR88}.  But, these and many similar worst-case hardness results are based on worst case training data assumptions. However, by using a result in \cite{klivans2006cryptographic} that learning a neural network with threshold activation functions is equivalent to learning intersection of halfspaces, several authors showed that under certain cryptographic assumptions, depth-two neural networks are not efficiently learnable with smooth activation functions \cite{LivniSS14, ZhangLWJ15, ZhangLJ15}.

%The main difficulty in analysis is the non-convexity of the loss objectives that deep learning present. Recent work has shown that SGD will efficiently converge to a local minimizer and escape saddle points, under modest assumptions \cite{GeHJY15}. Therefore, it suffices to analyze the local minima of the loss landscape and show that no spurious local minima exist. This direction has led to positive results in matrix sensing \cite{ParkKCS16a}, matrix completion \cite{GeLM16}, dictionary learning \cite{SunQW15}, phase retrieval \cite{SunQW16}, and orthogonal tensor decomposition \cite{GeHJY15}. 

 

Due to the difficulty of analysis of the non convex gradient descent in deep learning, many have turned to improper learning and the study of non-gradient methods to train neural networks. Janzamin et. al use tensor decomposition methods to learn the shallow neural network weights, provided access to the score function of the training data distribution \cite{JanzaminSA15}. Eigenvector and tensor methods are also used to train shallow neural networks with quadratic activation functions in \cite{LivniSS14}. Combinatorial methods that exploit layerwise correlations in sparse networks have also been analyzed provably in \cite{AroraBGM13}. Kernel methods, ridge regression, and even boosting were explored for regularized neural networks with smooth activation functions in \cite{shalev2011learning, ZhangLWJ15, ZhangLJ15}. Non-smooth activation functions, such as the ReLU, can be approximated by polynomials and are also amenable to kernel methods\cite{GoelKKT16}. These methods however are very different from the simple popular SGD.



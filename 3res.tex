
\section{Runtime Bounds with Stochastic Gradients}
\Anote{this section should start with a definition, then have a main theorem that provides, as you presage, ``runtime bounds with stochastic gradient''. as it stands, i can't find such a theorem. i see lots of little claims though. please provide a main theorem at the top, and rename everything else to ``lemma''.}

The gradient descent convergence results in the previous sections lay
the foundation for reasoning about the convergence of stochastic
gradient descent (SGD). To derive finite runtime bounds, we need to
address three technical details: 1) realizable potentials are only
approximately $\lambda$-harmonic, 2) the variance of the stochastic
gradient should be small and 3) SGD should escape local minima in a
bounded number of iterations. The second and third details are analyzed
with standard techniques in optimization and statistics; the former
requires generalization bounds for neural networks and the latter
requires a lower bound on the negative curvature. Together, we will
show that running SGD after $\poly(d,1/\epsilon)$ iterations on an
almost $\lambda$-harmonic potential will allow us to learn at least
one of the hidden weights $w_j$ up to $\epsilon$ accuracy.

%
We first state a lemma concerning the construction of an almost
$\lambda$-harmonic function on $S^{d-1}.$ The construction is given in
Section~\ref{App:EigenFunc} and is based on truncating the taylor
series expansion of a solution to the $\lambda$-harmonic differential
equation. This results in a bounded degree polynomial which
(approximately) satisfies the guarantees of $\lambda$-harmonic
potentials when $\theta$ is far from $w_j$. 
%
\begin{definition}
$\Phi(\theta, w)$ is an $m$-almost $\lambda$-harmonic on $S^{d-1}$ if for
all $\theta, w\in S^{d-1}$, $|\Delta_{\theta}\Phi(\theta, w) - \lambda \Phi (\theta,w)| \leq \poly(m)|\theta^Tw|^m$ 
\end{definition}
%
%
\begin{restatable}{lemma}{almostHarmonic}\label{AlmostHarmonic}
For any $m$ and $\lambda = O(\poly(m))$,  we can construct a realizable odd $m$-almost $\lambda$-harmonic potential, $\Phi_m$, that corresponds to a odd polynomial $\sigma$ of degree $m$. Furthermore, $|\Phi_m(\theta^Tw)|\leq |\theta^Tw|$ for all $\theta, w \in S^{d-1}$ and all first, second, and third partial derivatives of $\Phi_m$ are bounded by $\poly(m)$.
\end{restatable} 
%
To make sure that $\|a\|$ remains controlled throughout the optimization process, we add a quadratic regularization term to $L$ and instead run SGD on $G = L + \|a\|^2$. Then, we show that if $(\boldsymbol{a,\theta})$ is in $\mathcal{M}_{G, \epsilon}$ for $\epsilon$ small, then $\theta_i$ is close to $w_j$ for some $j$. Finally, we show how to initialize $(\boldsymbol{a^{(0)},\theta^{(0)}})$ and run SGD to converge to $\mathcal{M}_{G,\epsilon}$, proving our main theorem.
%
\begin{theorem}\label{eigSGD}
  Let $\mathcal{M} = S^{d-1}$ and $b_1,...,b_k$ be bounded by $\poly(d)$ and $w_1,..,w_k$ are randomly chosen from $\mathcal{M}$.  

  With high probability, for all $\epsilon \in (0,1),$ there exists $m, \delta, \alpha$ (that depends on $d$) such that for the $m$-almost $1$-harmonic potential $\Phi_m$ the
  following holds: we can chose an initial point $(\boldsymbol{a^{(0)}, \theta^{(0)}})$ so that if after running SGD (Algorithm \ref{SGD}) on the regularized objective
  $G(\boldsymbol{a,\theta})$ such that either 1) $G(\boldsymbol{a,\theta}) \leq \epsilon$ or 2) there exists an $i, j$ such that $|w_j^T\theta_i| > 1- \epsilon$ in $poly(d,1/\epsilon)$ iteration complexity and $d^{\poly(d, 1/\epsilon)}$ sample complexity.
\end{theorem}
%
%
%
Our goal is to analyze running SGD on $G$ (as in \eqref{errLoss}) with a $m$-almost $\lambda$-harmonic potential. By reusing techniques in \cite{GeHJY15}, we show that SGD (with noise) can avoid all points that are have a negative curvature of at least $\epsilon$ in $\poly(1/\epsilon)$ iterations, resulting in the following Lemma. Intuitively, this means that SGD will converge to points with small gradient and small negative curvature, converging to a point in $\mathcal{M}_{G, \epsilon}$, where 
%
\[\mathcal{M}_{G, \epsilon} = \left\{x\in \mathcal{M} \Big| \|\nabla G(x)\|
  \leq \epsilon \text{ and } \lambda_{min}(\nabla^2 G(x)) \geq
  -\epsilon\right\}\]
%
We note that the more straightforward algorithm of computing the negative descent direction of the Hessian is also a tractable algorithm to converge to $\mathcal{M}_{G,\epsilon}$. The full description and proof of our SGD algorithm is given in the supplementary material. 

%
\begin{restatable}{lemma}{strongConverge2}\label{strongConverge2}  
We can construct a $m$-almost $\lambda$-harmonic potential $\Phi_m$ for which
we can choose stepsize $\eta = 1/\poly(d,1/\epsilon,\log(1/\zeta))$, such that with probability at least $1-\zeta$, running Algorithm \ref{SGD} on $\widehat{G}$ initialized at $x_0$ with stepsize $\eta$  returns a point $x_\eta$ that is in $\mathcal{M}_{G, \epsilon}$ after at most $T = \poly(d,1/\epsilon, \log(1/\zeta))$ iterations. 
 Furthermore, $G(a,x_T) \leq G(a, x_0)$ with high probability. 
\end{restatable}
%
\begin{restatable}{lemma}{eigConv}
\label{eigConv}
Let $\mathcal{M} = S^{d-1}$ and let $b_1,...,b_k$ be reals bounded by
$\poly(d)$. For all $\epsilon \in (0,1),$ we can pick
$m = O(\log(d)\log(1/\delta)/\epsilon)$ such that the following holds:

Let $L$ be as in \eqref{errLoss} with an $m$-almost 1-harmonic
potential $\Phi_m$ and let $G = L + \|a\|^2$.  If
$(\boldsymbol{a,\theta}) \in \mathcal{M}_{G, \delta^2 / (4d)}$ and
$G(\boldsymbol{a,\theta}) \leq G(\boldsymbol{0, 0})$, then for all
$i$, either $|a_i| < \delta,$ or there exists $j$ such that
$|w_j^T\theta_i| > 1- k^2\epsilon.$ Furthermore,
unless all $|a_i| <\delta$, there exists $i, j$ such that
$|w_j^T\theta_i| > 1-\epsilon$.
\end{restatable}
%
\begin{proof}
  The proof is similar to Theorem \ref{EigStrict}. Consider all
  $\theta_i$ such that $|a_i| > \delta$ and
  $|w_j^T\theta_i| \leq 1-\epsilon$ for all $j$. WLOG, let these be
  $\theta_1,...,\theta_l$, for some $l \leq k$. Assume for now that
  furthermore, $|\theta_i^T\theta_j| \leq 1-\epsilon$ for
  $i \leq l, j\geq l+1$.  Then, consider a correlated rotation on the
  sphere, which is a correlated translation in spherical
  coordinates. So, consider the loss function 
%
\[ H({\bf a, v}) = G({\bf a,\theta_1+v,...,\theta_l+v},\theta_{l+1},..\theta_k)\]

Let $(\boldsymbol{a,\theta}) \in \mathcal{M}_{G, \delta^2/(4d)}$. Then,
the optimality conditions on ${\bf a}$ are as usual:
%
\begin{align*}
   \abs{\pd{H}{a_i}} & = \lvert 2\sum_{j=1}^k b_j \Phi_m(\theta_i^T w_j) +
    2\sum_{j: j\neq i} a_j \Phi_m(\theta_i^T\theta_j) \\
& \qquad \qquad \qquad + (2\Phi_m(1) +
    2)a_i \rvert \\
& \leq \delta 
\end{align*}
%
Since $\Phi_m$ is $m$-almost 1-harmonic, we get,
%
\begin{align*}
  &  \left| \Delta_{{\bf v}} H -  2\sum_{i=1}^l\sum_{j=1}^k a_i b_j
    \Phi_m(\theta_i^Tw_j) \right. \\
& \qquad \qquad \qquad \left.- 2\sum_{i=1}^l\sum_{j= l+1}^k
  a_ia_j(-\Phi_m(\theta_i^T\theta_j)) \right| \\
  & \leq   2\poly(m)\left(\sum_{i=1}^l\sum_{j=1}^k a_i b_j  |\theta_i^Tw_j|^m+\sum_{i=1}^l\sum_{j= l+1}^k a_ia_j|\theta_i^T\theta_j|^m\right)
\end{align*}
%
Together, we get
%
\begin{align*}
& |\Delta_{{\bf v}}H + \sum_{i=1}^l (2\Phi_m(\theta_i^T\theta_i)+2)a_i^2 + 2\sum_{i \neq j}^l a_ia_j\Phi_m(\theta_i^T\theta_j)| \\
& \leq \sum_{i=1}^l\delta |a_i| +\sum_{i=1}^l 2|a_i|\poly(m)|\sum_{j=1}^k b_j
  (\theta_i^Tw_j)^m +  \sum_{j=l+1}^k  a_j(\theta_i^T\theta_j)^m|
\\
\end{align*}

Again, since these inner products are assumed to be less than
$1-\epsilon$ and and $\|a\| \leq \poly(d)$ by
$G(\boldsymbol{a,\theta}) \leq G(\boldsymbol{0,0})$ and Lemma
\ref{bounded}, we can choose
$m = O(\frac{1}{\epsilon}\log(d)\log(1/\delta))$ large enough such
that
%
\begin{align*}
 2\poly(m)\sum_{i=1}^l|\sum_{j=1}^k b_j (\theta_i^Tw_j)^m +
  \sum_{j=l+1}^k  a_j(\theta_i^T\theta_j)^m| \\
 \leq  2\poly(d,k,m)e^{-\epsilon m} \leq \delta/2
\end{align*}

Finally, consider the following expression
%
\begin{align*}
D & = \expt\left[\left( \sum_{i=1}^l a_i \sigma(\theta_i,X)\right)^2\right] \\
%
& =\sum_{i=1}^l a_i^2\Phi_m(\theta_i^T\theta_i) + \sum_{i \neq j}^l
  a_ia_j\Phi_m(\theta_i^T\theta_j)
\end{align*}
%
Since $D \geq 0$ and combined with the fact that $|a_i| > \delta$,
%
\begin{align*}
\Delta_{{\bf v}}H & \leq  \sum_{i=1}^l (-2a_i^2 +\delta |a_i|+(\delta/2)|a_i|) \\
& \leq   \sum_{i=1}^l  |a_i|(-2\delta + 3\delta/2) < -l\delta^2/4
\end{align*}

Since the Laplacian is a sum of $d$ eigenvalues, this implies that we can find $u \in S^{d-1}$ such that $-l\delta^2/(4d) > u^T\nabla^2_v H u$. Finally, notice that if $w = (\underbrace{u,...u}_{l {\textrm{ times}}},0,...,0)$, then $w^T\nabla^2_{\boldsymbol{\theta}} G w = u^T\nabla^2_v H u < -l\delta^2/4d$. Since $w$ has $\|w\|^2 = l$, $\lambda_{min}(\nabla^2 G) < -\delta^2/(4d)$, contradicting that it is in $\mathcal{M}_{G, \delta^2/(4d)}$. 

Finally, it must be the case that there exists $\theta_{j_1},...\theta_{j_l}$ such that
$|\theta_i^T\theta_{j_1}| > 1-\epsilon$ and
$|\theta_{j_{i}}^T\theta_{j_{i+1}}| > 1-\epsilon$, and
$|\theta_{j_l}^Tw_j| > 1-\epsilon$ for some $w_j$. Therefore,
$|\theta_i^Tw_j| > 1- k^2\epsilon$ and unless all $|a_i| <\delta$,  there exists $i, j$ such that $|w_j^T\theta_i| > 1-\epsilon$.
\end{proof}

\begin{lemma}\label{eigRes}
  Assume the conditions of Lemma~\ref{eigConv}. If
$G({\bf a, \boldsymbol{\theta}}) \leq G(\boldsymbol{0,0}) - 2\delta\sqrt{G(\boldsymbol{0,0})}$
  and $(\boldsymbol{a,\theta}) \in \mathcal{M}_{G,\delta^2/(4\poly(d))}$,
  then there exists some $i, j$ such that $|\theta_i^Tw_j| > 1- \epsilon$.
\end{lemma}
 
 \begin{proof}
   By Lemma \ref{eigConv}, if there does not exists $i, j$ such that
   $|w_j^T\theta_i| > 1-\epsilon$, then for all $\theta_i$, then
   $|a_i| < \delta/\poly(d)$ for all $i$. Now, for a integrable
   function $f(x)$, $\| f\|_X = \sqrt{\expt_X[f(X)^2]}$ is a
   norm. Therefore, if $f(x) = \sum_i b_i \sigma(w_i,x)$ be our true
   target function, we conclude that by triangle inequality
\begin{align*}
\sqrt{G(\boldsymbol{a,\theta})}  & \geq \norm{\sum_{i=1}^k a_i \sigma(\theta_i,x) - f(x)}_X \\
&\geq \|f(x)\|_X\ - \sum_{i=1}^k \|a_i\sigma(\theta_i,x) \|_X \\
& \geq
  \sqrt{G(\boldsymbol{0,0})} - \delta
\end{align*}
Squaring gives a contradiction, so we conclude that there must exist $i, j$ such that $\theta_i$ is in a $\epsilon$ neighborhood of $w_j$.
 \end{proof}
 
 \begin{lemma}\label{initialize}
Assume the conditions of Lemma~\ref{eigConv} and let $\theta_1$ be chosen uniformly from $\mathcal{M}$. If $G(\boldsymbol{0,0}) \geq \epsilon$, then we can initialize $\boldsymbol{a^{(0)},\theta^{(0)}}$ such that $G({\bf a^{(0)}, \boldsymbol{\theta^{(0)}}}) \leq G(\boldsymbol{0,0}) - 2\delta\sqrt{G(\boldsymbol{0,0})}$ with $\delta = 1/\poly(d,1/\epsilon)$ with probability at least $e^{-O(d \log d)}$.
 \end{lemma}
 
 \begin{proof}
  Consider choosing $\theta_1$ uniformly from $\mathcal{M}$ and then
  optimizing $a_1$. Given $\theta_1$, the loss decrease is:
%
\begin{align*}
   G(a_1,\theta_1) - G(0,0) & = \min_{a_1} 2a_1^2 +
  2\sum_{j=1}^k a_1 b_j\Phi(\theta_1,w_j) \\
 %
 & = -\frac{1}{2}\left(  \sum_{j=1}^k b_j
   \Phi(\theta,w_j)\right)^2 
\end{align*}

Next, we can rewrite our loss as
%
\[G(\boldsymbol{0,0}) =  \sum_{i=1}^k\sum_{j=1}^k b_i b_j \Phi_m(w_i, w_j) \]

Therefore, we can find $i$, such that
$|\sum_{j=1}^k b_j \Phi_m(w_i, w_j)| \geq
G(\boldsymbol{0,0})/(kb_i)$. Now, notice that all first partials of $\Phi_m$ are $\poly(m)$-bounded, so $\Phi_m$ is $\poly(d, m)$- Lipschitz. Since $b_i$ is $\poly(d)$-bounded and $\Phi_m$ is $\poly(d,m)$-Lipschitz, we see
that in a $\Omega(1/poly(d,m))$-neighborhood of $w_i$,
%
\[|\sum_{j=1}^k b_j \Phi_m(\theta, w_j)| \geq G(\boldsymbol{0,0})/poly(d) \]

The probability $\theta$ randomly chosen would lie in this
neighborhood $\mathcal{N}$ is $e^{-O(d \log d)}$ (as the surface area of $S^{d-1}$ grow exponentially in $d$). So by choosing $\theta_1$ randomly and setting $a_1$ optimally and all the other $a_i$'s to $0$ with probability at least $e^{-O(d \log d)}$, we can initialize $(\boldsymbol{a^{(0)},\theta^{(0)}})$ such that 
%
\[G({\bf a^{(0)}, \boldsymbol{\theta^{(0)}}}) \leq G(\boldsymbol{0,0}) - \frac{1}{\poly(d)}G(\boldsymbol{0,0})^2\]

Finally, since $G(\boldsymbol{0,0}) \geq \epsilon$, our claim holds with $\delta = 1/\poly(d,1/\epsilon)$
\end{proof}
%

\if{1}

The following Lemma gives an alternate guarantee on $G({\bf a, \boldsymbol{\theta}})$ provided the $w$ are chosen randomly. 

 

\begin{lemma}\label{bigVariance}
Let $\Phi_m$ be as in Lemma \ref{AlmostHarmonic} and $b_1,...,b_k$ are $\poly(d)$-bounded. Let $\theta$ be uniformly randomly chosen on $S^{d-1}$. Then,
 
 $\expt\left[\left(  \sum_{j=1}^k b_j \Phi_m(\theta,w_j)\right)^2\right]
 \geq e^{-O(d)}G(\boldsymbol{0,0})$
\end{lemma}
\Snote{Richard, I don't think the details here are sufficient to
  understand the proof. Please elaborate, and fill in the missing details.}
\begin{proof}

We can rewrite our loss as
%
\[G(\boldsymbol{0,0}) =  \sum_{i=1}^k\sum_{j=1}^k b_i b_j \Phi_m(w_i, w_j) \]

Therefore, we can find $i$, such that
$|\sum_{j=1}^k b_j \Phi_m(w_i, w_j)| \geq
G(\boldsymbol{0,0})/(kb_i)$. Now, notice that all first partials of $\Phi_m$ are $\poly(m)$-bounded, so $\Phi_m$ is $\poly(d, m)$- Lipschitz. Since $b_i$ is $\poly(d)$-bounded and $\Phi_m$ is $\poly(d,m)$-Lipschitz, we see
that in a $\Omega(1/poly(d,m))$-neighborhood of $w_i$, call it $\mathcal{N}$,
%
\[|\sum_{j=1}^k b_j \Phi_m(\theta, w_j)| \geq G(\boldsymbol{0,0})/poly(d) \]

Note that $P(\theta \in \mathcal{N}) = e^{-O(d)}$ since the surface area of the sphere is exponential in $d$. Thus, we have

\[\expt[(  \sum_{j=1}^k b_j \Phi_m(\theta,w_j))^2]
 \geq \prob(\theta \in \mathcal{N})\expt[(\sum_{j=1}^k b_j \Phi(\theta, w_j))^2 | \theta \in \mathcal{N}]\]
\[\geq e^{-O(d)} L(\boldsymbol{0,0})\]
 \Snote{How does the claim
  about the square follow?} 
\end{proof}




\begin{lemma}\label{largeVariance}
  Let $\Phi_m$ be as in Lemma \ref{AlmostHarmonic} \Snote{Always say
    Lemma/Theorem when referring to one. So this should be
    Lemma~\ref{AlmostHarmonic}} and let $\theta, w_1,...,w_k$ be
  independent and uniformly randomly chosen on $S^{d-1}$. Then, over
  $\theta, w_1,...,w_k$,
 
 $\expt\left[\left(  \sum_{j=1}^k b_j \Phi(\theta,w_j)\right)^2\right]
 \geq \norm{b}_2^2 \cdot \Omega(1/d^m).$
\end{lemma}
%
\begin{proof}
Note that $\Phi_m$ is a degree $m$ polynomial with non-negative coefficients and it only has odd-degree terms. Since it is odd, $E[\Phi_m(\theta^Tw_i)] = 0$ and the cross terms in the expansion of the square vanish:
%
\[ \expt\left[\left(  \sum_{j=1}^k b_j
     \Phi_m(\theta^Tw_j)\right)^2\right] = \sum_{j=1}^k b_j^2 \expt[\Phi_m(\theta^Tw_j)^2]\]

So, it suffices to show $\expt[\Phi_m(\theta,w_j)^2] \geq 1/m$.  Now, since $\Phi_m$ has non-negative coefficients,
\begin{align*}
& \expt[\Phi_m(\theta^Tw_j)^2] = \expt\left[\left(\sum_{i=1, i \,
                               odd}^m c_i
                               (\theta^Tw_j)^i\right)^2\right] \\
& \qquad = \sum_{i,j \,odd} c_ic_j E[(\theta^Tw_j)^{i+j}] \geq \sum_{i,j
  \,odd}c_i c_j \Omega(1/(i+j))  \\
& \qquad =\Omega(1/d^m) 
\end{align*}

Where we used the fact that
$\sum_{i,j} c_ic_j = (\sum_i c_i)^2 = \Phi_m(1)^2 = 1$ and
$E[(\theta^Tw_j)^{n}] \geq \Omega(1/d^{n})$, which holds by noticing that with high probability $\theta^T w_j = \Omega(1/\sqrt{d})$. \Snote{The last claim is not correct. $(\theta^Tw_j)^n$ is
  $O(1/\sqrt{n}^n$ with high probability.}
\end{proof}

\fi 
 
\begin{proof}[Proof of Theorem \ref{eigSGD}]
Lemma~\ref{strongConverge2} ensures that we are in $\mathcal{M}_{G,\delta^2/(4\poly(d))}$ in $\poly(d,1/\epsilon)$ iterations.
If $G(\boldsymbol{0,0}) \leq \epsilon$, then by Lemma~\ref{strongConverge2}, with high probability, $G(\boldsymbol{a,\theta}) \leq G(\boldsymbol{0,0}) \leq \epsilon$. Otherwise, by Lemma \ref{initialize},  we can initialize $\boldsymbol{a^{(0)},\theta^{(0)}}$ such that $G(\boldsymbol{a^{(0)},\theta^{(0)}}) \leq  G(0,0) - 2\delta \sqrt{G(0,0)}$ for $\delta = 1/\poly(d,1/\epsilon)$ after $e^{O(d\log d)}$ samples by applying standard probabilistic bounds. Finally, we conclude by Theorem \ref{eigRes}.
\end{proof}
%

\if{1}

\Rnote{From here begins the reset algorithm...perhaps better with node-wise algorithm...}

In the end, we can further combine Theorem \ref{eigSGD} with Algorithm \ref{GDReset} to derive a convergence result that all $\theta_i$ will be close to some $w_j$.
 
 \begin{algorithm}[tb]
 \caption{SGD Algorithm with Resets}
   \label{GDReset}
\begin{algorithmic}
  \STATE {\bfseries Input:}
  $(\boldsymbol{a,\theta}) = (a_1,...,a_k,\theta_1,...,\theta_k), a_i
  \in\R, \theta_i\in\mathcal{M}$;
  $T\in \N$; $\widehat{L}$; $\alpha\in \R$; $\delta \in \R$;
  $\gamma \in R$; $\nu \in \R$; $\epsilon \in \R$ \vspace{0.1in} 
  
   \REPEAT
  \STATE $(\boldsymbol{a},\boldsymbol{\theta}) = SGD \left(\widehat{L} + \|a\|^2, (\boldsymbol{a},\boldsymbol{\theta}),T, \alpha,\delta \right)$
  \STATE Find $S \subset [k]$ maximal with $|a_j| < \nu$ for $j \in S$
  \IF{$|S| > 0$} 
     \FOR{$j \in S$} 
    \REPEAT \STATE Sample $\theta_j$
  uniformly from $\mathcal{M}$
  \UNTIL{$\left (\frac{\partial
        \widehat{L}(\boldsymbol{a,\theta_j})}{\partial a_j} \right)^2
    \geq \gamma$}
    \ENDFOR
  \ELSE
  \STATE  {\bf return} $(\boldsymbol{a}, \boldsymbol{\theta}) $
  \ENDIF
  \UNTIL{$\widehat{L}(\boldsymbol{a,\theta}) \leq \epsilon/2$}
   \STATE {\bf return} $(\boldsymbol{a}, \boldsymbol{\theta}) $
   \end{algorithmic}
\end{algorithm}

\begin{corollary}
Assume the conditions of Thm~\ref{eigSGD}. For any $0 < \epsilon < 1$, with high probability, running Algorithm \ref{GDReset} on $L$ with suitable parameters $\delta, \gamma, \alpha$ converges in $T = \poly(d, 1/\epsilon)$ iterations to $\boldsymbol{a,\theta}$ such that either $L(\boldsymbol{a,\theta}) \leq \epsilon$ or each $\theta_i$ is within $k^2\epsilon$-neighborhood of some $w_j$
\end{corollary}

\begin{proof}
If the algorithm returns $\boldsymbol{a,\theta}$ such that $\widehat{L}(\boldsymbol{a,\theta}) \leq \epsilon/2$, then since $\widehat{L}$ is a stochastic oracle for $L$ with error at most $\epsilon/2$, we conclude $L(\boldsymbol{a,\theta}) \leq \epsilon$. 

Otherwise, by Theorem \ref{eigSGD}, 
\end{proof}

\fi









%%% Local Variables:
%%% mode: latex
%%% TeX-master: "icmlpaper2017.tex"
%%% End:
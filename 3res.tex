
\section{Runtime Bounds with Stochastic Gradients}
\Anote{this section should start with a definition, then have a main theorem that provides, as you presage, ``runtime bounds with stochastic gradient''. as it stands, i can't find such a theorem. i see lots of little claims though. please provide a main theorem at the top, and rename everything else to ``lemma''.}

The gradient descent convergence results in the previous sections lay
the foundation for reasoning about the convergence of stochastic
gradient descent (SGD). To derive finite runtime bounds, we need to
address three technical details: 1) realizable potentials are only
approximately strictly subharmonic or $\lambda$-harmonic, 2) the
variance of the stochastic gradient and 3) SGD should escape local
minima in a bounded (hopefully polynomial) number of iterations. The
second and third detail are analyzed with standard techniques in
optimization and statistics: the former requires generalization bounds
for neural networks and the latter requires a lower bound on the
negative curvature, which leads to the follow definition.
%
\begin{definition} 
Let $\Omega \subseteq \R^d$. A point $x^*$ is a
  {\bf $\epsilon$-strict} point of $f : \Omega \to \R$ if f is twice
  differentiable at $x^*$ and
  $\lambda_{min}(\nabla^2f(x^*)) < -\epsilon$
\end{definition}

Note that if a critical point is not a local minimum, then it is
$0$-strict. By reusing techniques in \cite{GeHJY15}, we show that SGD (with noise) can avoid all points that are $\epsilon$-strict in $\poly(1/\epsilon)$ iterations. Intuitively, this means that SGD will converge to points with small gradient and small negative curvature, converging to a point in $\mathcal{M}_\epsilon$, where 
%
\[\mathcal{M}_{L, \epsilon} = \left\{x\in \mathcal{M} \Big| \|\nabla L(x)\|
  \leq \epsilon \text{ and } \lambda_{min}(\nabla^2 L(x)) \geq
  -\epsilon\right\}\]
%
We note that the more straightforward algorithm of computing the negative descent direction of the Hessian is possible. The full description and proof of our SGD algorithm is given in the supplementary material. 

We now discuss convergence results for a almost $\lambda$-harmonic function on $S^{d-1}$ that is constructed in the following theorem. The construction is technical and requires the Hermite polynomial basis in Theorem \ref{thm:rotReal}.
%
\begin{definition}
$\Phi(\theta, w)$ is $m$-almost $\lambda$-harmonic on $S^{d-1}$ if for
all $\theta, w\in S^{d-1}$, $|\Delta_{\theta}\Phi(\theta, w) - \lambda \Phi (\theta,w)| \leq \poly(m)|\theta^Tw|^m$ 
\end{definition}


\begin{restatable}{lemma}{almostHarmonic}\label{AlmostHarmonic}
For any $m$, we can construct a realizable odd $m$-almost 1-harmonic potential, $\Phi_m$, that corresponds to a odd polynomial $\sigma$ of degree $m$. Furthermore, $|\Phi_m(\theta^Tw)|\leq 1$ for all $\theta, w \in S^{d-1}$.
\end{restatable} \Snote{Can you generalize this theorem to $\lambda$-harmonic?}
%
%
\begin{restatable}{lemma}{bounded}
\label{bounded}
  Let $L$ be as in Eq.~\eqref{errLoss} and let $b_1,...,b_k$ be reals
  bounded by $\poly(d)$ and $|\Phi|\leq 1$ on $\mathcal{M}$. Let
  $G = L + \beta\|a\|^2$. Then, if
  $G(\boldsymbol{a,\theta}) \leq G(0,\boldsymbol{\theta})$, then
  $\|a\| \leq \poly(d)/\beta$.
\end{restatable}
%
%
\begin{restatable}{lemma}{eigConv}
\label{eigConv}
Let $\mathcal{M} = S^{d-1}$ and let $b_1,...,b_k$ be reals bounded by
$\poly(d)$. For all $\epsilon \in (0,1),$ we can pick
$m = O(\log(d)\log(1/\delta)/\epsilon)$ such that the following holds:

Let $L$ be as in \eqref{errLoss} with an $m$-almost 1-harmonic
potential $\Phi_m$ and let $G = L + \|a\|^2$.  If
$(\boldsymbol{a,\theta}) \in \mathcal{M}_{G, \delta^2 / (4d)}$ and
$G(\boldsymbol{a,\theta}) \leq G(0,\boldsymbol{\theta})$, then for all
$i$, either there exists $j$ such that
$|w_j^T\theta_i| > 1- k^2\epsilon,$ or $|a_i| < \delta$.
\end{restatable}
%
\begin{proof}
  The proof is similar to Theorem \ref{EigStrict}. Consider all
  $\theta_i$ such that $|a_i| > \delta$ and
  $|w_j^T\theta_i| \leq 1-\epsilon$ for all $j$. WLOG, let these be
  $\theta_1,...,\theta_l$, for some $l \leq k$. Assume for now that
  furthermore, $|\theta_i^T\theta_j| \leq 1-\epsilon$ for
  $i \leq l, j\geq l+1$.  Then, consider a correlated rotation on the
  sphere, which is a correlated translation in spherical
  coordinates. So, consider the loss function 
%
\[ H({\bf a, v}) = G({\bf a,\theta_1+v,...,\theta_l+v},\theta_{l+1},..\theta_k)\]

Let $(\boldsymbol{a,\theta}) \in \mathcal{M}_{G, \delta^2/(4d)}$. Then,
the optimality conditions on ${\bf a}$ are as usual:
%
\begin{align*}
   \abs{\pd{H}{a_i}} & = \lvert 2\sum_{j=1}^k b_j \Phi_m(\theta_i^T w_j) +
    2\sum_{j: j\neq i} a_j \Phi_m(\theta_i^T\theta_j) \\
& \qquad \qquad \qquad + (2\Phi_m(1) +
    2)a_i \rvert \\
& \leq \delta 
\end{align*}
%
Since $\Phi_m$ is $m$-almost 1-harmonic, we get,
%
\begin{align*}
  &  \left| \Delta_{{\bf v}} H -  2\sum_{i=1}^l\sum_{j=1}^k a_i b_j
    \Phi_m(\theta_i^Tw_j) \right. \\
& \qquad \qquad \qquad \left.- 2\sum_{i=1}^l\sum_{j= l+1}^k
  a_ia_j(-\Phi_m(\theta_i^T\theta_j)) \right| \\
  & \leq   2\poly(m)\left(\sum_{i=1}^l\sum_{j=1}^k a_i b_j  |\theta_i^Tw_j|^m+\sum_{i=1}^l\sum_{j= l+1}^k a_ia_j|\theta_i^T\theta_j|^m\right)
\end{align*}
%
Together, we get
%
\begin{align*}
& |\Delta_{{\bf v}}H + \sum_{i=1}^l (2\Phi_m(\theta_i^T\theta_i)+2)a_i^2 + 2\sum_{i \neq j}^l a_ia_j\Phi_m(\theta_i^T\theta_j)| \\
& \leq \sum_{i=1}^l\delta |a_i| +\sum_{i=1}^l 2|a_i|\poly(m)|\sum_{j=1}^k b_j
  (\theta_i^Tw_j)^m +  \sum_{j=l+1}^k  a_j(\theta_i^T\theta_j)^m|
\\
\end{align*}

Again, since these inner products are assumed to be less than
$1-\epsilon$ and and $\|a\| \leq \poly(d)$ by
$G(\boldsymbol{a,\theta}) \leq G(0,\boldsymbol{\theta})$ and Lemma
\ref{bounded}, we can choose
$m = O(\frac{1}{\epsilon}\log(d)\log(1/\delta))$ large enough such
that
%
\begin{align*}
 2\poly(m)\sum_{i=1}^l|\sum_{j=1}^k b_j (\theta_i^Tw_j)^m +
  \sum_{j=l+1}^k  a_j(\theta_i^T\theta_j)^m| \\
 \leq  2\poly(d,k,m)e^{-\epsilon m} \leq \delta/2
\end{align*}

Finally, consider the following expression
%
\begin{align*}
D & = \expt\left[\left( \sum_{i=1}^l a_i \sigma(\theta_i,X)\right)^2\right] \\
%
& =\sum_{i=1}^l a_i^2\Phi_m(\theta_i^T\theta_i) + \sum_{i \neq j}^l
  a_ia_j\Phi_m(\theta_i^T\theta_j)
\end{align*}
%
Since $D \geq 0$ and combined with the fact that $|a_i| > \delta$,
%
\begin{align*}
\Delta_{{\bf v}}H & \leq  \sum_{i=1}^l (-2a_i^2 +\delta |a_i|+(\delta/2)|a_i|) \\
& \leq   \sum_{i=1}^l  |a_i|(-2\delta + 3\delta/2) < -l\delta^2/4
\end{align*}

Since the Laplacian is a sum of $d$ eigenvalues, this implies that we can find $u \in S^{d-1}$ such that $-l\delta^2/(4d) > u^T\nabla^2_v H u$. Finally, notice that if $w = (\underbrace{u,...u}_{l {\textrm{ times}}},0,...,0)$, then $w^T\nabla^2_{\boldsymbol{\theta}} G w = u^T\nabla^2_v H u < -l\delta^2/4d$. Since $w$ has $\|w\|^2 = l$, we conclude that $(\boldsymbol{a,\theta})$ is $\delta^2/(4d)$-strict, contradicting that it is in $\mathcal{M}_{G, \delta^2/(4d)}$. 

Finally, it must be the case that there exists $\theta_{j_1},...\theta_{j_l}$ such that
$|\theta_i^T\theta_{j_1}| > 1-\epsilon$ and
$|\theta_{j_{i}}^T\theta_{j_{i+1}}| > 1-\epsilon$, and
$|\theta_{j_l}^Tw_j| > 1-\epsilon$ for some $w_j$. Therefore,
$|\theta_i^Tw_j| > 1- k^2\epsilon$. 
\end{proof}

\begin{theorem}\label{eigRes}
  Assume the conditions of Theorem~\ref{eigConv}. If
$G({\bf a, \boldsymbol{\theta}}) \leq G(0,\boldsymbol{\theta}) - 2\delta\sqrt{G(0,\boldsymbol{\theta})}$
  and $(\boldsymbol{a,\theta}) \in \mathcal{M}_{G,\delta^2/(4\poly(d))}$,
  then there exists some $i, j$ such that $\theta_i$ is in an
  $\epsilon$-neighborhood of $w_j$.
\end{theorem}
 
 \begin{proof}
   By Lemma \ref{eigConv}, if there does not exists $i, j$ such that
   $|w_j^T\theta_i| > 1-k^2\epsilon$ is if for all $\theta_i$, then
   $|a_i| < \delta/\poly(d)$ for all $i$. Now, for a integrable
   function $f(x)$, $\| f\|_X = \sqrt{\expt_X[f(X)^2]}$ is a
   norm. Therefore, if $f(x) = \sum_i b_i \sigma(w_i,x)$ be our true
   target function, we conclude that by triangle inequality
\begin{align*}
\sqrt{G(\boldsymbol{a,\theta})}  & \geq \norm{\sum_{i=1}^k a_i \sigma(\theta_i,x) - f(x)}_X \\
&\geq \|f(x)\|_X\ - \sum_{i=1}^k \|a_i\sigma(\theta_i,x) \|_X \\
& \geq
  \sqrt{G(0,\boldsymbol{\theta})} - \delta
\end{align*}
Squaring gives a contradiction, so we conclude that there must exist $i, j$ such that $\theta_i$ is in a $k^2\epsilon$ neighborhood of $w_j$.
 \end{proof}
 
 \begin{observation}\label{initialize}
Let $\theta_1$ be chosen uniformly from $\mathcal{M}$ and assume
\[ \expt\left[\left(  \sum_{j=1}^k b_j \Phi(\theta_1,w_j)\right)^2\right] > 5\delta\sqrt{L(0,0)}\] 

Then, we can initialize $\boldsymbol{a,\theta}$ such that $G({\bf a, \boldsymbol{\theta}}) \leq G(0,\boldsymbol{\theta}) - 2\delta\sqrt{G(0,\boldsymbol{\theta})}$ with high probability.
 \end{observation}
 
 \begin{proof}
   Consider choosing $\theta_1$ uniformly from $\mathcal{M}$ and then
   optimizing $a_1$. Notice that the expected loss decrease is:
%
\begin{align*}
  \expt[ G(a_1,\theta_1)] - G(0,0) & = \expt\left [\min_{a_1} 2a_1^2 +
  2\sum_{j=1}^k a_1 b_j\Phi(\theta_1,w_j)\right] \\
 %
 & = -\frac{1}{2}\expt\left[\left(  \sum_{j=1}^k b_j
   \Phi(\theta,w_j)\right)^2\right] 
\end{align*}
Let
$h(\theta) = \sum_i b_i \Phi(\theta, w_i)$. Since $a_i,b_i, \Phi$ are
$\poly(d)$-bounded, our random variable $h(\theta)$ is
$\poly(d)$-bounded. From Hoeffding bounds, we see that after
$\poly(d,1/\delta)$ samples, we know that
$\hat{\expt}h(\theta)^2 \geq 2\delta\sqrt{G(0,0)}$, where
$\hat{\expt}$ denotes the empirical average over $\theta$. Therefore,
we must have found some $\theta$ such that
$h(\theta)^2 \geq 2\delta \sqrt{G(0,0)}$. Therefore,
$G(\boldsymbol{a,\theta}) - G(0,0) \leq 2\delta \sqrt{G(0,0)}$.
\end{proof}
 
\begin{lemma}\label{largeVariance}
  Let $\Phi_m$ be as in \ref{AlmostHarmonic} \Snote{Always say
    Lemma/Theorem when referring to one. So this should be
    Lemma~\ref{AlmostHarmonic}} and let $\theta, w_1,...,w_k$ be
  independent and uniformly randomly chosen on $S^{d-1}$. Then, over
  $\theta, w_1,...,w_k$,
 
 $\expt\left[\left(  \sum_{j=1}^k b_j \Phi(\theta,w_j)\right)^2\right]
 \geq \norm{b}_2^2 \cdot \Omega(1/m).$
\end{lemma}
%
\begin{proof}
Note that $\Phi_m$ is a degree $m$ polynomial with non-negative coefficients and it only has odd-degree terms. Since it is odd, $E[\Phi_m(\theta^Tw_i)] = 0$ and the cross terms in the expansion of the square vanish:
%
\[ \expt\left[\left(  \sum_{j=1}^k b_j
     \Phi_m(\theta^Tw_j)\right)^2\right] = \sum_{j=1}^k b_j^2 \expt[\Phi_m(\theta^Tw_j)^2]\]

So, it suffices to show $\expt[\Phi_m(\theta,w_j)^2] \geq 1/m$.  Now, since $\Phi_m$ has non-negative coefficients,
\begin{align*}
& \expt[\Phi_m(\theta^Tw_j)^2] = \expt\left[\left(\sum_{i=1, i \,
                               odd}^m c_i
                               (\theta^Tw_j)^i\right)^2\right] \\
& \qquad = \sum_{i,j \,odd} c_ic_j E[(\theta^Tw_j)^{i+j}] \geq \sum_{i,j
  \,odd}c_i c_j \Omega(1/(i+j))  \\
& \qquad =\Omega(1/m) 
\end{align*}

Where we used the fact that
$\sum_{i,j} c_ic_j = (\sum_i c_i)^2 = \Phi_m(1)^2 = 1$ and
$E[(\theta^Tw_j)^{n}] \geq \Omega(1/n)$, which holds by integration by
parts. \Snote{The last claim is not correct. $(\theta^Tw_j)^n$ is
  $O(1/\sqrt{n})^n$ with high probability.}
\end{proof}

\begin{theorem}
  Let $\mathcal{M} = S^{d-1}$ and let $b_1,...,b_k$ be such that
  $||b||_2$ is $\Omega(1)$ and $O(poly(d))$, and $w_1,..,w_k$ are
  randomly chosen from $\mathcal{M}$. 
  
  With high probability, for all $\epsilon \in (0,1),$ there exists $m, \delta$ (that depends on $d$) such that for the $m$-almost $1$-harmonic potential $\Phi_m$ the
  following holds: we can chose an initial point $(\boldsymbol{a^{(0)}, \theta^{(0)}})$ so that if after running SGD (Algorithm \ref{SGD}) on the regularized objective
  $G(\boldsymbol{a,\theta})$, there exists an $i, j$ such that $|w_j^T\theta_i| > 1- k^2\epsilon$ in $O(poly(d,1/\epsilon))$ iteration complexity and $O(poly(d^{log(d)},1/\epsilon))$ sample complexity.
\end{theorem}

\begin{proof}
We choose $m = O(\log d \log (1/\delta)/\epsilon)$. Combining Observation \ref{initialize} and Lemma \ref{largeVariance}, we can initialize $\boldsymbol{a,\theta}$ such that $G(\boldsymbol{a,\theta}) - G(0,0) \leq 2\delta \sqrt{G(0,0)}$ for $\delta = O(1/m) = \poly(d,1/\epsilon)$ \Rnote{This is not exactly true but correct in expectation..to apply concentration, we need to sample $w_i$ multiple times}.

Next, to use Theorem \ref{strongConverge}, we check the regularity conditions. By assumption, we can choose $B, L, \rho$ to be $\poly(d,1/\epsilon)$ since $\Phi$ and the second and third partials of $\Phi$ are all bounded by $\poly(d,1/\epsilon)$ in $\mathcal{M}$. Furthermore, our activation function $\sigma$ and its derivatives are bounded in magnitude by $c|x|^{m}$, where $m = O(\log d \log (1/\delta)/\epsilon)$ and by \cite{Hermite}, we can let $c = O(m^m)$. By Theorem \ref{genErrBound}, with high probability, we can construct a stochastic oracle up to $\epsilon$ error with sample complexity $\poly(d^{\log(d)},1/\epsilon)$. Therefore, we conclude by Theorem \ref{strongConverge} that we are in $\mathcal{M}_{G,\delta^2/(4\poly(d))}$ in $\poly(d,1/\epsilon)$ iterations. Finally, we conclude by Theorem \ref{eigRes}.
\end{proof}











 \if{1}
 In the end, we can combine Theorem \ref{eigRes} with Algorithm \ref{GDReset} to derive a convergence result.
 
 \begin{algorithm}[tb]
 \caption{SGD Algorithm with Resets}
   \label{GDReset}
\begin{algorithmic}
  \STATE {\bfseries Input:}
  $(\boldsymbol{a,\theta}) = (a_1,...,a_k,\theta_1,...,\theta_k), a_i
  \in\R, \theta_i\in\mathcal{M}$;
  $T\in \N$; $\widehat{L}$; $\alpha\in \R$; $\delta \in \R$;
  $\gamma \in R$; $S \in \R$ \vspace{0.1in} 
  \FOR{$i=1$ {\bfseries to} $S$} 
  \STATE $(\boldsymbol{a},\boldsymbol{\theta}) = SGD \left(\widehat{L}, (\boldsymbol{a},\boldsymbol{\theta}),T, \alpha,\delta \right)$
  \IF{(ii) or (iii)}
    \STATE remove clump
    \REPEAT \STATE Sample $\theta_i$
  uniformly from $\mathcal{M}$
  \UNTIL{$\left (\frac{\partial
        \widehat{L}(\boldsymbol{a,\theta})}{\partial a_i} \right)^2
    \geq \gamma$}
    \ELSE \STATE  {\bf return} $(\boldsymbol{a}, \boldsymbol{\theta}) $
      \ENDIF
    \ENDFOR
   \STATE {\bf return} $(\boldsymbol{a}, \boldsymbol{\theta}) $
   \end{algorithmic}
\end{algorithm}

\begin{theorem}
For any $0 < \epsilon < 1$, we can construct a realizable potential $\Phi$ such that with high probability, running Algorithm \ref{GDReset} on \eqref{errLoss} with error $\delta = \poly(\epsilon,1/d)$, $\gamma = \epsilon$ and stepsize $\alpha = 1/\poly(d,1/\epsilon)$ converges in $T = \poly(d, 1/\epsilon)$ iterations to $(\boldsymbol{a,\theta})$ such that each $\theta_i$ is within $\epsilon$-neighborhood of some $w_j$ or there exists $i$ such that if $\theta_i$ is picked uniformly in $\mathcal{M}$
%
\[ \expt\left[\left( \sum_{j < i} a_j \Phi(\theta_i,\theta_j) + \sum_{j=1}^k b_j \Phi(\theta_i,w_j)\right)^2\right] < \epsilon\]
\end{theorem}

\begin{proof}


\end{proof}
\fi


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "icmlpaper2017.tex"
%%% End:
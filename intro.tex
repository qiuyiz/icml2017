\section{Introduction}


Deep learning has resulted in major strides in machine learning applications including speech recognition, image classification, and ad-matching. The simple idea of using multiple layers of nodes with a non-linear activation function at each node allows one to express any function.  To learn a certain target function we just use (stochastic) gradient descent to minimize the loss; this approach has resulted in significantly lower error rates for several real world functions, such as those in the above applications. Naturally the question remains: how close are we to the optimal values of the network weight parameters? Are we stuck in some bad local minima? While there are several recent works \cite{ChoromanskaHMAL14, DauphinPGCGB14, Kawaguchi16a} that have tried to study the presence of local minima the picture is far from clear.

There has been some work on studying how well can neural networks learn some synthetic function classes (e.g. polynomials~\cite{valiant2014learning}, decision trees). 
In this work we study how well can neural networks learn neural networks with gradient descent?
% In this work we ask how well can deep learning learn certain types of synthetic functions. 
% Simple examples of synthetic functions are. 
% Here we will study this question for functions that can be represented by a deep network of a certain depth and width. 
%
 % Can we use neural networks to learn neural networks with gradient descent methods? 
 % gradient descent on neural networks learn a randomly initialized target network with the same shape.
%
 Specifically, if the target function is a neural network with randomly initialized weights, and we attempt to learn it using a network with the same architecture, then will gradient descent converge to the target function?

% If a target function is deep network with a certain shape and has randomly initialized edge weights, then will gradient descent converge to the target function?
% {\color{red}That is, if the function to be learnt is a neural network, and we try to learn it with a network of the same shape and randomly initialized edge weights, then will the gradient descent converge to the right function? }




Experimental simulations show that for short depths (say two), and for different widths, with random network weights, stochastic gradient descent of a hypothesis network with the same architecture converges to an $\ell_2$ error that is a small percentage of a random network, indicating that SGD can learn these shallow networks with random weights (see section~\ref{experiments}). 

% Our experimental simulations show that for different widths and heights functions represented by neural networks with random edge weights can be learnt by stochastic gradient descent.
 % Our experimental simulations show that for short depths (say 2) and with random edge weights the error rate drops to a small percentage. 
%To understand this better, 



\paragraph{Results and Contributions.} 
We theoretically investigate the question of convergence for networks of depth two.
 % with certain simiplifying assumptions.  
Our main conceptual contribution is that for depth $2$ networks where the top node is a sum node, the question of whether gradient descent converges to the desired target function is equivalent to the following question in electrodynamics: Given $n$ fixed charges in $\rea^d,$ and $n$ mobile chages,
% initialized at random positions 
with all the mobile charges moving under the influence of the 
attractive force between opposite-sign charges, and repulsive force between  same-sign charges,
% electrical force of attraction from the protons and repulsion from the remaining electrons. 
at equilibrium, are all the mobile charges matched up with all the fixed charges, upto a permutation? 

In the above, $n$ corresponds to the number of hidden units, $d$ is the number of inputs to the  network, the positions of each fixed charge corresponds to the input weight vector of
% a  edge weights incident on each of the
a hidden unit in the target network, and the initial positions of the mobile charges are the initial values of the edge weights for the hidden units at the beginning of the gradient descent in the learning network. The motion of the charges essentially tracks the change in the network during gradient descent. The force between a pair of charges is not given by the standard electrical force of $1/r^2$ (where $r$ is the distance between unit charges), but by a function determined by the activation and the input distribution. Thus the question of convergence in these simplified depth two networks can be resolved by studying the equivalent electrodynamics question with the corresponding force function.
%
% \Anote{If you're going to have an informal theorem statement, they should be understandable without reading the main text. That's because a reader like me will ignore your intro text and just look at the theorem statements to gauge your paper. If the theorems look interesting, then they'll read the paper. This theorem refers to f, which is undefined. it also doesn't explain at all in what way the electrons and protons correspond to f. You should either make this explicit in the informal theorem's statement, or you should just blend this informal theorem into the rest of the section.}
\begin{theorem}[informal statement of Theorem~\ref{EPDyn}]
Applying gradient descent for learning the output of a depth two network
with activation $\sigma$ in the hidden layer and a linear output
node, under squared loss, 
% two layer
% neural network 
% The gradient descent process for learning our neural network $f$ 
is equivalent to the motion of k charges in the presence of k fixed
charges where the force between any pair of charges is given by a
potential function that depends on $\sigma$ and the input distribution.
\end{theorem}
%

 % and that the training data comes from a standard multivariate Gaussian distribution.
Our main technical contribution is to prove the existence of an activation function such that the corresponding gradient descent dynamics under standard Gaussian inputs result in learning at least one of the hidden nodes in the target network. We then show that this allows us to learn the complete target network one node at a time. We leave open the problem of convergence for forces corresponding to more realistic activation functions. We assume the sample complexity is close to its infinite limit. 
%
% {\color{red} }
%We also derive the force function for several possible activation functions. }
% Further for a certain synthetic activation function, we prove that the electrodynamic force function results in convergence thus 
% implying the desired convergence for simplified depth 2 networks with those activation functions 

%
% \Anote{same problem as the other informal theorem. i don't know what a $\theta$ is. it has only been defined in the text. furthermore, it's far too informal. instead of "carefully" it should give this notion a name. again, better to just blend this content in the rest of the text without calling it an informal theorem.}
\begin{theorem}[informal statement of Theorem~\ref{almostHarmSGD}]
There is an activation function such that running gradient
  descent for minimizing the squared loss along with $\ell_2$
  regularization for standard Gaussian inputs, at convergence, 
% For some
%   carefully chosen activation function, with regularization, at
%   convergence of running the SGD algorithm, 
  we learn at least one of
  the hidden weights of the target neural network.
\end{theorem}


We  prove that the above result can be iterated to learn the entire network node-by-node using gradient descent (Theorem~\ref{nodeWise}).  Our algorithm learns a network with the same architecture and number of hidden nodes as the target network. This is in contrast to the improper learning setting of many proposed algorithms.

In the supplementary material, we show some weak results for more practical activations. For the sign activation, we show that for the loss with respect to a single node, the only local minima are at the hidden target nodes with high probability if the target network has a randomly picked top layer. For the polynomial activation, we derive a similar result under the assumption that the hidden nodes are orthonormal and show a polynomial convergence of an SGD algorithm to learn these weights.


\paragraph{Intuition and Techniques.}
%
Note that for the standard electric potential function given by $\Phi = 1/r$ where $r$ is the distance between the charges, it is known from Earnshaw's theorem that an electrodynamic system with some fixed charges and some moving electrons is at equilibrium only when the moving charges coincide with the fixed charges. Given our translation above between electrodynamic systems and depth 2 networks (Section~\ref{sec:epdyn}), this would imply learnability of depth 2 networks with gradient descent under $\ell_2$ loss, if the activation function corresponds to the electrostatic potential. However, there exists no activation function $\sigma$ corresponding to this $\Phi$. 
%

% \paragraph{Organization.} In section 2, we introduce our framework and assumptions, and derive the equivalence between gradient descent and electron-proton dynamics under a suitable potential. 
%  % These convergence results are proven to simply illustrate our ideas. 
% In Section~\ref{}, we construct a realizable almost $\lambda$-harmonic potential and prove finite convergence guarantees. In section 6, experimental results confirm that depth-2 neural networks can be learned by gradient descent with common activation functions but seem to discredit that claim for higher depth networks. In section 5, we consider more realistic activation functions, such as the sign and polynomial function.

The proof of Earnshaw's theorem is based on the fact that the electrostatic potential is harmonic, \emph{i.e}, its Laplacian (trace of its Hessian) is identically zero. This ensures that at every critical point, there is direction of potential reduction (unless the hessian is identically zero). We generalize these ideas to potential functions that are eigenfunctions of the Laplacians, $\lambda$-harmonic potentials (Section~\ref{sec:earnshaw}). However, there potentials are unbounded. Subsequently, we construct a non-explicit activation function such that the corresponding potential is bounded and is almost $\lambda$-harmonic, \emph{i.e.}, it is $\lambda$-harmonic outside a small sphere (Section~\ref{sec:almost-harmonic}). For this activation function, we show at a stable critical point, we must learn at least one of the hidden nodes. Gradient descent (possibly with some noise, as in~\cite{GeHJY15}) is believed to converge to stable critical points. However, for simplicity, we use second order information to escape saddle points.

%\Snote{We use recent results on gradient descent~\cite{Rongetal} to show that gradient descent (with perturbations) converges close a stable critical point.}

{\color{red}The use of second-order methods is not limiting since noisy gradient descent algorithms can descent along negative curvature directions. Therefore, stochastic gradient descent should also converge to $M_{G,\epsilon}$ \cite{GeHJY15}, although we lack some regularity conditions. Furthermore, a more controlled perturbed gradient descent \cite{jin2017escape} can be applied in our setting to reach $M_{G,\epsilon}$ but requires more in-depth analysis.}


% Our main tool for analysis is to derive second-order information about our dynamics by using the Laplacian of the Hessian (or a submatrix of the Hessian) of our loss function. 
% Together with some generalization error bounds and discrete optimization results, we can finally translate these convergence results into finite time convergence rates. 


% We derive some sufficient conditions that characterize potentials
%  that arise from activation functions $\sigma$. 

% The different $\sigma$ that we study, their corresponding potentials, and their convergence results are summarized in Table ~\ref{table1}.
% First we show how to construct an activation function, such that the corresponding potential satisfies a nice smooth property which we call almost $\lambda$-harmonic, 
% for which we show that at convergence of the gradient descent, at least some $\theta_i$ coincides with some $w_j$.
%

%
%
% \begin{table}[tb]
% \caption{Activation, Potentials, and Convergence Results Summary}
% \label{table1}
% \noindent
% \vskip 0.1in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{
%   |p{\dimexpr.3\linewidth-2\tabcolsep-1.3333\arrayrulewidth}% column 1
%   |p{\dimexpr.37\linewidth-2\tabcolsep-1.3333\arrayrulewidth}% column 2
%   |p{\dimexpr.33\linewidth-2\tabcolsep-1.3333\arrayrulewidth}|% column 3
%   }
%    \hline 
%         Name of Activation&  Potential  ($\Phi(\theta,w)$)    & Convergence? \\ \hline 
%         Sign & $1 - \frac{2}{\pi}\cos^{-1}(\theta^Tw)$       & Yes d = 2, \ref{signCon} \ref{SignConv}\\ 
%         Polynomial  & $(\theta^Tw)^m$       & Yes, $\poly(d,\frac{1}{\epsilon}) \ref{PolyConv}$  \\        
%         Almost   $\lambda$-harmonic  & Poly($\theta^Tw$), \ref{AlmostHarmonic}  & Yes, $\poly(d,\frac{1}{\epsilon})$ \\
%          % Bessel    &  $e^{-\|\theta-w\|_1}$        & Yes for $d=1$  \\   
%         \hline
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table} 
% %


We acknowledge that there is still a large gap between our developed theory and practice. However, our work can offer theoretical explanations and guidelines for the design of better activation functions or gradient-based training algorithms. For example, better accuracy and training speed were reported when using the newly discovered exponential linear unit (ELU) activation function in \cite{ClevertUH15, ShahKSS16}. We hope for more theory-backed answers to these and many other questions in deep learning.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "icmlpaper2017.tex"
%%% End:

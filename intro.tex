\section{Introduction}


Deep learning has resulted in major strides in machine learning applications including speech recognition, image classification, and ad-matching. The simple idea of using multiple layers of nodes with a non-linear activation function at each node allows one to express any function.  To learn a certain target function we just use (stochastic) gradient descent to minimize the loss; this approach has resulted in significantly lower error rates for several real world functions, such as those in the above applications. Naturally the question remains: how close are we to the optimal values of the network weight parameters? Are we stuck in some bad local minima? While there are several recent works \cite{ChoromanskaHMAL14, DauphinPGCGB14, Kawaguchi16a} that have tried to study the presence of local minima the picture is far from clear.

There has been some work on studying how well can neural networks learn some synthetic function classes (e.g. polynomials~\cite{valiant2014learning}, decision trees). 
In this work we study how well can neural networks learn neural networks with gradient descent?
% In this work we ask how well can deep learning learn certain types of synthetic functions. 
% Simple examples of synthetic functions are. 
% Here we will study this question for functions that can be represented by a deep network of a certain depth and width. 
%
 % Can we use neural networks to learn neural networks with gradient descent methods? 
 % gradient descent on neural networks learn a randomly initialized target network with the same shape.
%
 Specifically, if the target function is a neural network with randomly initialized weights, and we attempt to learn it using a network with the same architecture, then will gradient descent converge to the target function?

% If a target function is deep network with a certain shape and has randomly initialized edge weights, then will gradient descent converge to the target function?
% {\color{red}That is, if the function to be learnt is a neural network, and we try to learn it with a network of the same shape and randomly initialized edge weights, then will the gradient descent converge to the right function? }




Experimental simulations show that for short depths (say two), and for different widths, with random network weights, stochastic gradient descent of a hypothesis network with the same architecture converges to an $\ell_2$ error that is a small percentage of a random network, indicating that SGD can learn these shallow networks with random weights (see section~\ref{experiments}). 

% Our experimental simulations show that for different widths and heights functions represented by neural networks with random edge weights can be learnt by stochastic gradient descent.
 % Our experimental simulations show that for short depths (say 2) and with random edge weights the error rate drops to a small percentage. 
%To understand this better, 
We theoretically investigate this question of convergence for networks of depth two with certain simiplifying assumptions. Specifically, we assume that the top node is a sum node instead of a non linear node and that the training data comes from a standard multivariate Gaussian distribution. 



\paragraph{Results and Contributions.} Our main conceptual contribution is that for these simplified depth $2$ networks, the question of whether the gradient descent converges to the desired target function is equivalent to a certain question of convergence in electrodynamics: given $n$ fixed protons~\Snote{charges?} in $d$ dimensional Euclidean space and $n$ electrons initialized at random positions with all the electrons moving under the influence of the electrical force of attraction from the protons and repulsion from the remaining electrons. The question of convergence, then, is whether at equilibrium all the electrons will be matched up all the protons upto some permutation. Here $n$ corresponds to the number of hidden units, $d$ is the number of inputs to the deep network, the positions of each proton corresponds to the input weight vector of
% a  edge weights incident on each of the
a hidden unit in the target network, and the initial positions of the electrons are the initial values of the edge weights for the hidden units at the beginning of the gradient descent in the learning network. The motion of the electrons essentially tracks the change in the network during gradient descent. The force function between a pair of charges is not given by the standard electrical force of $1/r^2$ (where $r$ is the distance between unit charges), but by another function that is determined by the activation function and the distribution of inputs. Thus the question of convergence in these simplified depth two networks can be resolved by studying the equivalent electrodynamics question with the corresponding force function.
%
% \Anote{If you're going to have an informal theorem statement, they should be understandable without reading the main text. That's because a reader like me will ignore your intro text and just look at the theorem statements to gauge your paper. If the theorems look interesting, then they'll read the paper. This theorem refers to f, which is undefined. it also doesn't explain at all in what way the electrons and protons correspond to f. You should either make this explicit in the informal theorem's statement, or you should just blend this informal theorem into the rest of the section.}
\begin{theorem}(informal statement of Theorem~\ref{EPDyn})
Applying gradient descent for learning the output of a neural network
with one hidden layer and a linear activation function at the output
node, under standard Gaussian inputs and squared loss, 
% two layer
% neural network 
% The gradient descent process for learning our neural network $f$ 
is equivalent to the motion of k electrons in the presence of k fixed
protons where the force between any pair of charges is given by a
potential function that depends on $\sigma$.
\end{theorem}
%

Our main technical contribution is to prove the existence of an activation function such that the corresponding gradient descent dynamics result in learning at least one of the hidden nodes in the target network. We then show that this allows us to learn the complete target network one node at a time. We leave open the problem of convergence for forces corresponding to more realistic activation functions. We assume the sample complexity is close to its infinite limit. 
%
% {\color{red} }
%We also derive the force function for several possible activation functions. }
% Further for a certain synthetic activation function, we prove that the electrodynamic force function results in convergence thus 
% implying the desired convergence for simplified depth 2 networks with those activation functions 

%
% \Anote{same problem as the other informal theorem. i don't know what a $\theta$ is. it has only been defined in the text. furthermore, it's far too informal. instead of "carefully" it should give this notion a name. again, better to just blend this content in the rest of the text without calling it an informal theorem.}
\begin{theorem}[informal statement of Theorem~\ref{almostHarmSGD}]
There is an activation function such that upon running gradient
  descent for minimizing the squared loss along with $\ell_2$
  regularization for standard Gaussian inputs, at convergence, 
% For some
%   carefully chosen activation function, with regularization, at
%   convergence of running the SGD algorithm, 
  we learn at least one of
  the hidden weights of the target neural network.
\end{theorem}



\paragraph{Intuition and Techniques.}
%
Note that for the standard electric potential function given by $\Phi = 1/r$ where $r$ is the distance between the charges, it is known from Earnshaw's theorem that an electrodynamic system with some fixed charges and some moving electrons is at equilibrium only when the moving charges coincide with the fixed charges. Given our translation above between electrodynamic systems and depth 2 networks (Section~\ref{sec:epdyn}), this would imply learnability of depth 2 networks with gradient descent under $\ell_2$ loss, if the activation function corresponds to the electrostatic potential. However, there exists no activation function $\sigma$ corresponding to this $\Phi$. 
%

% \paragraph{Organization.} In section 2, we introduce our framework and assumptions, and derive the equivalence between gradient descent and electron-proton dynamics under a suitable potential. 
%  % These convergence results are proven to simply illustrate our ideas. 
% In Section~\ref{}, we construct a realizable almost $\lambda$-harmonic potential and prove finite convergence guarantees. In section 6, experimental results confirm that depth-2 neural networks can be learned by gradient descent with common activation functions but seem to discredit that claim for higher depth networks. In section 5, we consider more realistic activation functions, such as the sign and polynomial function.

The proof of Earnshaw's theorem is based on the fact that the electrostatic potential is harmonic, \emph{i.e}, its Laplacian (trace of its Hessian) is identically zero. This ensures that at every critical point, there is direction of potential reduction (unless the hessian is identically zero). We generalize these ideas to potential functions that are eigenfunctions of the Laplacians, $\lambda$-harmonic potentials (Section~\ref{sec:lambda}). However, there potentials are unbounded. Subsequently, we construct a non-explicit activation function such that the corresponding potential is bounded and is almost $\lambda$-harmonic, \emph{i.e.}, it is $\lambda$-harmonic outside a small sphere (Section~\ref{}). For this activation function, we show at a stable critical point, we must learn at least one of the hidden nodes. 

%\Snote{We use recent results on gradient descent~\cite{Rongetal} to show that gradient descent (with perturbations) converges close a stable critical point.}

{\color{red}The use of second-order methods is not limiting since noisy gradient descent algorithms can descent along negative curvature directions. Therefore, stochastic gradient descent should also converge to $M_{G,\epsilon}$ \cite{GeHJY15}, although we lack some regularity conditions. Furthermore, a more controlled perturbed gradient descent \cite{jin2017escape} can be applied in our setting to reach $M_{G,\epsilon}$ but requires more in-depth analysis.}



% Our main tool for analysis is to derive second-order information about our dynamics by using the Laplacian of the Hessian (or a submatrix of the Hessian) of our loss function. 
% Together with some generalization error bounds and discrete optimization results, we can finally translate these convergence results into finite time convergence rates. 
We remark that our algorithms learn and return a neural network with the same architecture and number of hidden nodes as the target network. This is a big contrast to the improper learning setting of many proposed algorithms. 


% We derive some sufficient conditions that characterize potentials
%  that arise from activation functions $\sigma$. 

% The different $\sigma$ that we study, their corresponding potentials, and their convergence results are summarized in Table ~\ref{table1}.
% First we show how to construct an activation function, such that the corresponding potential satisfies a nice smooth property which we call almost $\lambda$-harmonic, 
% for which we show that at convergence of the gradient descent, at least some $\theta_i$ coincides with some $w_j$.
%

%
In the supplementary material, we also show some weak results for other more practical activation functions. Specifically, for the sign activation, we show that if we consider the loss function with respect to a single variable $\theta_i$, then the only local minimas are at the true weights $w_j$, with high probability under random initializations of $b_i$. For the polynomial activation, we derive a similar result under the assumption that our weights $w_j$ are orthonormal and show convergence of a SGD algorithm to learn these weights in $\poly(d,1/\epsilon)$ time.
%
% \begin{table}[tb]
% \caption{Activation, Potentials, and Convergence Results Summary}
% \label{table1}
% \noindent
% \vskip 0.1in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{
%   |p{\dimexpr.3\linewidth-2\tabcolsep-1.3333\arrayrulewidth}% column 1
%   |p{\dimexpr.37\linewidth-2\tabcolsep-1.3333\arrayrulewidth}% column 2
%   |p{\dimexpr.33\linewidth-2\tabcolsep-1.3333\arrayrulewidth}|% column 3
%   }
%    \hline 
%         Name of Activation&  Potential  ($\Phi(\theta,w)$)    & Convergence? \\ \hline 
%         Sign & $1 - \frac{2}{\pi}\cos^{-1}(\theta^Tw)$       & Yes d = 2, \ref{signCon} \ref{SignConv}\\ 
%         Polynomial  & $(\theta^Tw)^m$       & Yes, $\poly(d,\frac{1}{\epsilon}) \ref{PolyConv}$  \\        
%         Almost   $\lambda$-harmonic  & Poly($\theta^Tw$), \ref{AlmostHarmonic}  & Yes, $\poly(d,\frac{1}{\epsilon})$ \\
%          % Bessel    &  $e^{-\|\theta-w\|_1}$        & Yes for $d=1$  \\   
%         \hline
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table} 
% %


We acknowledge that there is still a large gap between our developed theory and practice. However, our work can offer theoretical explanations and guidelines for the design of better activation functions or gradient-based training algorithms. For example, better accuracy and training speed were reported when using the newly discovered exponential linear unit (ELU) activation function in \cite{ClevertUH15, ShahKSS16}. We hope for more theory-backed answers to these and many other questions in deep learning.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "icmlpaper2017.tex"
%%% End:

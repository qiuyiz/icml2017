
\section{Realizable Potentials with Convergence Guarantees}

In this section, we derive convergence guarantees for a class of realizable potentials that closely approximate $\lambda$-harmonic potentials. First, we construct realizable potentials with corresponding activation functions that are approximately $\lambda$-harmonic, specifically they are $\lambda$-harmonic outside of a small neighborhood around the center. We will defer the details of the technical construction to the appendix. Then, we reason similarly about the Laplacian of our loss function to derive our convergence theorem. To make sure that $\|a\|$ remains controlled throughout the optimization process, we add a quadratic regularization term to $L$ and instead run SGD on $G = L + \beta\|a\|^2$. 

By reusing techniques in \cite{GeHJY15}, we show that SGD (with noise) can avoid all points that are have a negative curvature of at least $\epsilon$ in $\poly(1/\epsilon)$ iterations. Intuitively, this means that SGD will converge to points with small gradient and small negative curvature, converging to a point in $\mathcal{M}_{G, \epsilon}$, where 
%
%
\[\mathcal{M}_{G, \epsilon} = \left\{x\in \mathcal{M} \Big| \|\nabla G(x)\|
  \leq \epsilon \text{ and } \lambda_{min}(\nabla^2 G(x)) \geq
  -\epsilon\right\}\]
%
Then, we show that if $(\boldsymbol{a,\theta})$ is in $\mathcal{M}_{G, \epsilon}$ for $\epsilon$ small, then $\theta_i$ is close to $w_j$ for some $j$. Finally, we show how to initialize $(\boldsymbol{a^{(0)},\theta^{(0)}})$ and run SGD to converge to $\mathcal{M}_{G,\epsilon}$, proving our following theorem.
%
\begin{algorithm}[hb]
 \caption{$x = SGD(\widehat{L}, x_0, T,\alpha,\epsilon)$}
   \label{SGD}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\widehat{L}:\mathcal{M} \to \R$; $x_0 \in \mathcal{M}$; $T\in \N$; $\alpha \in \R$; $\epsilon\in\R$
   \vspace{.1in}
   \STATE {\bf Initialize} $x = x_0$
   \FOR{$i=1$ {\bfseries to} $T$}
   \STATE Sample $\omega$ uniformly on unit sphere.
   \STATE $x = x - \alpha(\nabla \widehat{L} (x)+\omega)$ 
   \STATE $x = \Pi_\mathcal{M} x$
   \IF{$\|\nabla\widehat{L}(x)\|\leq 2\epsilon/3$ {\bf and} $\lambda_{min}(\nabla^2\widehat{L}(x)) \geq -2\epsilon/3$}
   \STATE {\bf return} $x$
   \ENDIF
   \ENDFOR
   \STATE {\bf return} FAIL
\end{algorithmic}
\end{algorithm}
%

\begin{theorem}\label{almostHarmSGD}
  Let $\mathcal{M} = \R^{d}$. For all $\epsilon \in (0,1),$ there exists an activation $\sigma$ such that if $w_1,...,w_k \in \R^d$ with $w_i$ randomly chosen from $w_i \sim  \mathcal{N}({\bf 0}, O(d\log k){\bf I_{d\times d}})$ and $b_1,...,b_k$ be randomly chosen at uniform from $[-1,1]$, then with high probability, we can choose an initial point $(\boldsymbol{a^{(0)}, \theta^{(0)}})$ such that after running SGD (Algorithm \ref{SGD}) on the regularized objective $G(\boldsymbol{a,\theta})$ for at most $\poly(1/\beta,1/\lambda)d^{\sqrt{\lambda}}(d/\epsilon)^{O(d)}$ iterations, there exists an $i, j$ such that $\|\theta_i - w_j\| <  \epsilon$.
\end{theorem}


We start by stating a lemma concerning the construction of an approximately
$\lambda$-harmonic function on $\R^d.$ The construction is given in
Section~\ref{realizable} and is quite technical.
%
%
\begin{restatable}{lemma}{AlmostHarmReal}\label{almostHarmReal}
Let $\mathcal{M} = \R^d$ for $d \equiv 3 \mod 4$. Then, for any $1 > \epsilon > 0$, we can construct a realizable normalized radial potential $\Phi_\epsilon(r)$ that is $\lambda$-harmonic when $r \geq \epsilon$.

Furthermore, we have ${\Phi_\epsilon}^{(d-1)}(r) \geq 0$ for all $r  > 0$ and ${\Phi_\epsilon}^{(k)}(r) \geq 0$ and ${\Phi_\epsilon}^{(k+1)}(r)\leq 0$ for all $r > 0$ and $d - 3 \geq k \geq 0 $ even.

Lastly, $|{\Phi}_\epsilon^{(k)}(r)| \leq 3(2d + \sqrt{\lambda})^{2d} \epsilon^{-2d}e^{\sqrt{\lambda}}$ for all $0 \leq k \leq d-1$. And for $r \geq \epsilon$, $e^{-\sqrt{\lambda}r}r^{2-d}(2d+\sqrt{\lambda})^{-2d}\epsilon^{2d}/3\leq {\Phi}_\epsilon(r) \leq (1+r\sqrt{\lambda})^de^{\sqrt{\lambda}(1-r)}(r)^{2-d}$. Also for $r \geq \epsilon$, $ e^{-\sqrt{\lambda}r}r^{1-d}(2d+\sqrt{\lambda})^{-2d}\epsilon^{2d}/3 \leq |{\Phi}_\epsilon'(r)| \leq (d+\sqrt{\lambda}r)(1+ r\sqrt{\lambda})^de^{\sqrt{\lambda}(1- r)} r^{1-d}$
\end{restatable}
%
%
%
\begin{lemma}\label{almostHarmConv}
Let $\mathcal{M} = \R^d$ for $d \equiv 3 \mod 4$ and let $L$ be as in \eqref{errLoss} corresponding to the activation function $\sigma$ given by Lemma~\ref{almostHarmReal}. For any $\epsilon, \delta > 0$, we can construct $\sigma$ such that if $\boldsymbol{(a,\theta)} \in \mathcal{M}_{G,\delta}$, then for all $i$, either 1) there exists $j$ such that $\|\theta_i - w_j\| < \epsilon$ or 2) $a_i^2 < \delta/(\beta\lambda)$, (as long as $\beta \geq \delta$)
\end{lemma}
%
\begin{proof}
 The proof is similar to Theorem \ref{EigStrict}. Let $\Phi_\epsilon$ be the realizable potential in \ref{almostHarmReal} such that $\Phi_\epsilon(r)$ is $\lambda$-harmonic when $r \geq \epsilon/k$. Note that $\Phi_\epsilon(0) = 1$ is normalized. And let $\boldsymbol{(a,\theta)} \in \mathcal{M}_{G,\delta}$. 
 
WLOG, consider $\theta_1$ and a initial set $S_0 = \{ \theta_1\}$ containing it. For a finite set of points $S$ and a point $x$, define $d(x,S) = \min_{y \in S} \| x - y\|$. Then, we consider the following set growing process. If there exists $\theta_i, w_i \not \in S_j$ such that $d(\theta_i, S_j) < \epsilon/k$ or $d(w_i, S_j) < \epsilon/k$, add $\theta_i, w_i$ to $S_j$ to form $S_{j+1}$. Otherwise, we stop the process. We grow $S_0$ to until the process terminates and we have the grown set $S$.

If there is some $w_j \in S$, then it must be the case that there exists ${j_1},\cdots {j_q}$ such that $\|\theta_1 - \theta_{j_1} \| < \epsilon/k$ and
$\|\theta_{j_{i}} - \theta_{j_{i+1}}\| < \epsilon/k$, and
$\|\theta_{j_q}- w_j\| <\epsilon/k$ for some $w_j$. So, there exists $j$, such that $\|\theta_1 - w_j\| < \epsilon$. 

Otherwise, notice that for each $\theta_i \in S$, $\|w_j - \theta_i\|\geq \epsilon/k$ for all $j$, and $\|\theta_i - \theta_j\| \geq \epsilon/k$ for all $\theta_j\not \in S$. WLOG, let $S = \{\theta_1,\dots,\theta_l\}$. 
  
We consider changing all
$\theta_1, \ldots, \theta_{l}$ by the same $v$ and define 
%
\[H({\bf a}, v) = G({\bf a},\theta_1+v,...,\theta_l+v, \theta_{l+1}
\ldots, \theta_k).\]

The optimality conditions on ${\bf a}$ are 
\begin{align*}
   \abs{\pd{H}{a_i}} & = \lvert (2+2\beta)a_i  + 2\sum_{j\neq i} a_j \Phi_\epsilon(\theta_i,\theta_j) + 2\sum_{j=1}^k b_j \Phi_\epsilon(\theta_i,w_j) \rvert \\
& \leq \delta
\end{align*}
%
Next, since $\Phi_\epsilon(r)$ is $\lambda$-harmonic for $r \geq \epsilon/k$, we may calculate the Laplacian of $H$ as
%
\begin{align*}
\Delta_v H & = \sum_{i=1}^l \lambda \left(2\sum_{j=1}^k a_ib_j
  \Phi_\epsilon(\theta_i, w_j) + 2\sum_{j=l+1}^k a_ia_j
  \Phi_\epsilon(\theta_i, \theta_j)\right) \\
& = \sum_{i=1}^l \lambda \left(-2\beta a_i^2 -2a_i^2 - 2
  \sum_{j = 1, j\neq i}^l  a_ia_j \Phi_\epsilon(\theta_i,\theta_j)\right) \\
  & \qquad \qquad  + \delta \sum_{i=1}^l \lambda a_i^2 \\
&= -2\lambda\expt\left[\left( \sum_{i=1}^l a_i \sigma(\theta_i,X)\right)^2\right] -2\beta\lambda \sum_{i=1}^l a_i^2\\
& \qquad \qquad + \delta\lambda\sum_{i=1}^l  a_i^2 \\
\end{align*} 
%
The second line follows from our optimality conditions and the third line follows from completing the square. Since $\boldsymbol{(a,\theta)} \in \mathcal{M}_{G,\delta}$, we have $\Delta_v H \geq - \delta$. Because $\beta \geq \delta$, we see that we must satisfy $\sum_{i=1}^l a_i^2 \leq \delta/(\beta\lambda)$. Therefore, $a_i^2 \leq \delta/(\beta\lambda)$.

\end{proof}

\begin{lemma}\label{almostHarmRes}
  Assume the conditions of Lemma~\ref{almostHarmConv}. If
$G({\bf a, \boldsymbol{\theta}}) \leq G(\boldsymbol{0,0}) - \delta\sqrt{G(\boldsymbol{0,0})}$
  and $(\boldsymbol{a,\theta}) \in \mathcal{M}_{G,\beta\lambda\delta^2/2k^2}$,
  then there exists some $i, j$ such that $\|\theta_i - w_j\| <\epsilon$.
\end{lemma}
 
 \begin{proof}
 If there does not exists $i, j$ such that
   $\|\theta_i - w_j\| <\epsilon$, then by Lemma \ref{almostHarmConv}, this implies $a_i^2 < \delta^2/2k^2$ for all $i$. Now, for a integrable
   function $f(x)$, $\| f\|_X = \sqrt{\expt_X[f(X)^2]}$ is a
   norm. Therefore, if $f(x) = \sum_i b_i \sigma(w_i,x)$ be our true
   target function, we conclude that by triangle inequality
\begin{align*}
\sqrt{G(\boldsymbol{a,\theta})}  & \geq \norm{\sum_{i=1}^k a_i \sigma(\theta_i,x) - f(x)}_X \\
&\geq \|f(x)\|_X\ - \sum_{i=1}^k \|a_i\sigma(\theta_i,x) \|_X \\
& \geq
  \sqrt{G(\boldsymbol{0,0})} - \delta/2
\end{align*}
Squaring gives a contradiction, so we conclude that there must exist $i, j$ such that $\theta_i$ is in a $\epsilon$ neighborhood of $w_j$.
 \end{proof}
 
 \begin{lemma}[Initialization]\label{almostHarmInitialize}
Assume the conditions of Theorem~\ref{almostHarmSGD} and Lemma~\ref{almostHarmConv}. With high probability, we can initialize $\boldsymbol{(a^{(0)},\theta^{(0)})}$ such that $G({\bf a^{(0)}}, \boldsymbol{\theta^{(0)}}) \leq G(\boldsymbol{0,0}) - 2\delta\sqrt{G(\boldsymbol{0,0})}$ with $\delta = \frac{1}{1+\beta}d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{ - O(d)}$.
 \end{lemma}
 
 \begin{proof}
  Consider choosing $\theta_1 = {\bf 0}$ and then
  optimizing $a_1$. Given $\theta_1$, the loss decrease is:
%
\begin{align*}
   G(a_1,{\bf 0}) - G({\bf 0},{\bf 0}) & = \min_{a_1} (1+\beta)a_1^2 +
  2\sum_{j=1}^k a_1 b_j\Phi_\epsilon({\bf 0},w_j) \\
 %
 & = -\frac{1}{1+\beta}\left(  \sum_{j=1}^k b_j
   \Phi_\epsilon({\bf 0},w_j)\right)^2 
\end{align*}

Because $w_j$ are random Gaussians with variance $O(d \log k)$, we have $\|w_j\| \leq O(d\log k)$ with high probability for all $j$. By Lemma~\ref{almostHarmReal}, our potential satisfies ${\Phi}_\epsilon({\bf 0}, w_j) \geq d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{ - O(d)}$. And since $b_j$ are uniformly chosen in $[-1,1]$, we conclude that with high probability over the choices of $b_j$, $-\frac{1}{2}\left(  \sum_{j=1}^k b_j\Phi(\theta_1,w_j)\right)^2 \geq d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{ - O(d)}$ by appealing to Chebyshev's inequality for uniform variables (separate lemma?).

Therefore, we conclude that with high probability, $G(a_1, {\bf 0}) \leq G(\boldsymbol{0,0}) - \frac{1}{1+\beta}d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{ - O(d)}$. Since $G(\boldsymbol{0,0}) \leq O(k)$, we are done. 


%%Cramer-Rao type Argument
\if{1}
Let $f(x) =  \sum_{j=1}^k b_j \Phi(x,w_j)$, then it suffices (***Justify later) to show that $\Var(f(X)) \geq 1/\poly(d) $, where $X$ is a multivariate standard normal. By appealing to Cramer-Rao type bounds given in \cite{cacoullos1982upper}, we see that

\begin{align*}
 \Var(f(X)) &\geq \frac{1}{d}\expt \left[\sum_{i=1}^d \frac{\partial}{\partial x_i} f(X)\right]^2 \\
 &= \frac{1}{d}\left( \sum_{j=1}^k b_j \sum_{i=1}^d \expt \frac{\partial}{\partial x_i}\Phi(X,w_j) \right)^2 \\
\end{align*}

By Stein's identity, $\expt[\frac{\partial}{\partial x_i}\Phi(X,w_j)] = \expt[X_i\Phi(X,w_j)]$. And since $\Phi$ is translationally symmetric, if we let $w_{ji}$ be the $i$-th coordinate of $w_j$, then $\expt[ X_i\Phi(X,w_j)] = \expt[(X_i - w_{ji})\Phi(X, {\bf 0})] = -w_{ji}\expt[\Phi(X,{\bf 0})] = -Cw_{ji}$, where $C$ is a positive constant and $C\geq 1/\poly(d)$ (justify later).

Therefore, $\Var(f(X)) \geq \frac{C^2}{d} \left(\sum_{j=1}^k b_j \sum_{i=1}^d w_{ji}\right)^2$. However, note that $w_{ji}$ are independent Gaussians of variance 1, so we conclude that $\sum_{j=1}^k b_j \sum_{i=1}^d w_{ji}$ is a Gaussian of variance $d\|{\bf b}\|^2 \geq d$. So, with high probability over $w_1,...,w_j$, we conclude that $\Var(f(X)) \geq C^2 \geq 1/\poly(d)$.
\fi
\end{proof}
%



\begin{proof}[Proof of Theorem \ref{almostHarmSGD}]
First, by Lemma~\ref{almostHarmInitialize},  we can initialize $\boldsymbol{(a^{(0)},\theta^{(0)})}$ such that $G(\boldsymbol{a^{(0)},\theta^{(0)}}) \leq  G({\bf 0,0}) - 2\delta \sqrt{G({\bf 0,0})}$ for $\delta = d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{-O(d)}$. Next, Lemma~\ref{almostHarmReal} ensures all first three derivatives of $\Phi$ are bounded by $(d/\epsilon)^{O(d)}$, except at $w_1,...,w_k$. Appealing to Theorem~\ref{strongConverge}, our algorithm returns a point $\boldsymbol{(a,\theta)}$ in $\mathcal{M}_{G,\beta\lambda\delta^2/2k^2}$ in $\poly(1/\beta,1/\lambda)d^{\sqrt{\lambda}}(d/\epsilon)^{O(d)}$ iterations and furthermore, $G(\boldsymbol{a,\theta}) \leq G({\bf 0,0}) - 2\delta \sqrt{G({\bf 0,0})}$ with high probability. Finally, we conclude by Theorem \ref{almostHarmRes}.
\end{proof}

\subsection{Node-by-Node Analysis}

We cannot easily analyze the convergence of GD to the global minima when all $\theta_i$ are simultaneously moving since the pairwise interaction terms between the $\theta_i$ present complications. To
derive a tighter control on $(\boldsymbol{a,\theta})$, we run a greedy
node-wise SGD algorithm to learn the hidden weights, i.e. we run a
full SGD algorithm with respect to $(a_i,\theta_i)$ sequentially. The
main idea is that after running SGD with respect to $\theta_1$,
$\theta_1$ should be close to some $w_j$ for some $j$. Then, we can
carefully induct and show that $\theta_2$ must be some other $w_k$ for
$k\neq j$ and so on. 

%
\begin{algorithm}[tb]
 \caption{Node-wise Gradient Descent Algorithm with Output Weights Optimization}
   \label{NodeGDOpt}
\begin{algorithmic}
  \STATE {\bfseries Input:}
  $(\boldsymbol{a,\theta}) = (a_1,...,a_k,\theta_1,...,\theta_k), a_i
  \in\R, \theta_i\in\mathcal{M}$;
  $T\in \N$; $\widehat{L}$; $\alpha\in \R$; $\delta \in \R$;
  $\gamma \in R$; \vspace{0.1in} \STATE{\bf Initialize} $a = 0$
  \FOR{$i=1$ {\bfseries to} $k$} 
  \REPEAT \STATE Sample $\theta_i$
  uniformly from $\mathcal{M}$
  \UNTIL{$\left (\frac{\partial
        \widehat{L}(\boldsymbol{a,\theta})}{\partial a_i} \right)^2
    \geq \gamma$}
  \FOR{$j=1$ {\bfseries to} $T$} \STATE
  $a_i = a_i^* = GradientDescent \left(\widehat{L}_{a_i}, a_i, 1,
    \frac{\partial^2 \hat{L}}{\partial a_i^2} \right)$
  \STATE
  $\theta_i = SGD \left(\widehat{L}_{\theta_i}, \theta_i,1, \alpha,\delta \right)$
   \ENDFOR
   \STATE    $a =  GradientDescent \left(\widehat{L}_{a_1,..,a_i},
     (a_1,..,a_i), T , \alpha \right)$\;
   \ENDFOR
   \STATE {\bf return} $a = (a_1,...,a_k), \theta = (\theta_1,..., \theta_k)$
   \end{algorithmic}
\end{algorithm}

To apply our node-wise guarantees, we would first need to first tighten the convergence properties of SGD in the case where we have only 1 variable charge, say $\theta_1$. Let $G_1(a_1,\theta_1)$ be the regularized objective $G$ restricted to $a_1,\theta_1$ being variable, and $a_2,...,a_k = 0$ are fixed. While our previous guarantees before allow us to reach a $\epsilon$-neighborhood of $w_j$ when running SGD on $G_1$, we actually would like to reach a $(d/\epsilon)^{-O(d)}$-neighborhood of $w_j$. This is accomplished by reasoning about the first derivatives of our potential, mainly by showing that in an $\epsilon$-neighborhood of $w_j$, descending along the gradient always brings $\theta_1$ closer to $w_j$.



\begin{lemma}\label{nodewiseSGD}
Let $\mathcal{M} = \R^{d}$. For all $\epsilon \in (0,1),$ there exists an activation $\sigma$ such that if $w_1,...,w_k \in \R^d$ with $w_i$ randomly chosen from $w_i \sim  \mathcal{N}({\bf 0}, O(d\log k){\bf I_{d\times d}})$ and $b_1,...,b_k$ be randomly chosen at uniform from $[-1,1]$, then with high probability, we can choose an initial point $(a^{(0)}, \theta^{(0)})$ such that after running SGD (Algorithm \ref{SGD}) on the restricted regularized objective $G_1(a,\theta)$ for at most $\poly(1/\beta,1/\lambda)d^{\sqrt{\lambda}}(d/\epsilon)^{O(d)}$ iterations, there exists some $w_j$ such that $\|\theta - w_j\| < O(d/\epsilon)^{-O(d)}$ and $|a - b_j| < (d/\epsilon)^{-O(d)}$
\end{lemma}


\begin{proof}
First, by Lemma~\ref{almostHarmInitialize},  we can initialize ${(a^{(0)},\theta^{(0)})}$ such that $G({a^{(0)},\theta^{(0)}}) \leq  G({\bf 0,0}) - 2\delta \sqrt{G({\bf 0,0})}$ for $\delta = d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{-O(d)}$. Next, Lemma~\ref{almostHarmReal} ensures all first three derivatives of $\Phi$ are bounded by $(d/\epsilon)^{O(d)}$, except at $w_1,...,w_k$. Now, consider running SGD to generate a sequence $x_1,...,x_T$ with step-sizes of $(d/\epsilon)^{-O(d)}$ so that each gradient step moves at most a distance of $(d/\epsilon)^{-O(d)}$. 

If the SGD iterates $x_i$, for all $i$, is not in a $(d/\epsilon)^{-O(d)}$ neighborhood of $w_1,...,w_k$, then appealing to Theorem~\ref{strongConverge}, our algorithm returns a point $\boldsymbol{(a,\theta)}$ in $\mathcal{M}_{G,(d/\epsilon)^{-O(d)}}$ in $\poly(1/\beta,1/\lambda)d^{\sqrt{\lambda}}(d/\epsilon)^{O(d)}$ iterations and furthermore, $G(\boldsymbol{a,\theta}) \leq G({\bf 0,0}) - 2\delta \sqrt{G({\bf 0,0})}$ with high probability. Finally, we conclude by Theorem \ref{almostHarmRes} that there exists $w_j$ such that $\|\theta - w_j\| < \epsilon$.

Now, our gradient with respect to $\theta$ is
%
\begin{align*}
\nabla_\theta G_1 &=  2\sum_{i=1}^k a b_i \nabla_\theta \Phi_\epsilon(\|\theta - w_i\|) \\
&= 2ab_j \Phi_\epsilon'(\|\theta - w_j\|) \frac{\theta - w_j}{\|\theta - w_j\|} \\
&\qquad+ 2\sum_{i\neq j} ab_i\Phi_\epsilon'(\|\theta - w_i\|) \frac{\theta - w_i}{\|\theta - w_i\|}
\end{align*}
%

By our construction, since $\|\theta - w_j\| \leq \epsilon$, we may lower bound $|\Phi_\epsilon'(\|\theta - w_j\|)| \geq e^{-\sqrt{\lambda}\epsilon}\epsilon^{1-d}(2d+\sqrt{\lambda})^{-2d}\epsilon^{2d}/3 \geq (d/\epsilon)^{-2d}$. On the other hand, for all $i \neq j$, we note that with high probability $\|w_i - w_j\| \geq \Omega(d \log k)$. Therefore, we may upper bound $|\Phi_{\epsilon}(\|\theta - w_i\|)| \leq 1/\poly(k)(d/\epsilon)^{-O(d)}$. Finally, we note that all $|b_i | \geq 1/\poly(k)$ with high probability. 

Together, we conclude that $\nabla_\theta G_1 = 2ab_j \Phi_\epsilon'(\|\theta - w_j\|) \frac{\theta - w_j}{\|\theta - w_j\|} + 2a\eta$, where $\|\eta\| \leq (d/\epsilon)^{-O(d)} b_j \Phi_\epsilon'(\|\theta - w_j\|) \frac{\theta - w_j}{\|\theta - w_j\|}$. 

Similarly, since $(a,\theta) \in \mathcal{M}_{G, (d/\epsilon)^{-O(d)}}$, we know that

\begin{align*}
   \abs{\pd{G}{a}} & = \lvert (2+2\beta)a  + 2b_j \Phi_\epsilon(\|\theta - w_j\|) \\
    &+ 2\sum_{i \neq j} b_i \Phi_\epsilon(\|\theta - w_i\|) \rvert \leq (d/\epsilon)^{-O(d)}
\end{align*}

By a similar argument as on the derivative, we see that $a = -2b_j \Phi_\epsilon(\|\theta - w_j\|) + O(d/\epsilon)^{-O(d)}$ and $|b_j \Phi_\epsilon(\|\theta - w_j\|) | \geq (d/\epsilon)^{-2d}$. By combining our observations, we realize that since SGD is moving in the direction of $-\nabla_\theta G_1$, it is moving $\theta$ closer to $w_j$ since 

\begin{align*}
\nabla_\theta G_1 &=  2ab_j \Phi_\epsilon'(\|\theta - w_j\|) \frac{\theta - w_j}{\|\theta - w_j\|} + (d/\epsilon)^{-O(d)} \\
&= -2b_j^2\Phi_\epsilon(\|\theta-w_j\|)\Phi_\epsilon'(\|\theta - w_j\|) \frac{\theta - w_j}{\|\theta - w_j\|} \\
&\qquad + (d/\epsilon)^{-O(d)} 
\end{align*}

However, we see that with high probability, $|b_j^2\Phi_\epsilon(\|\theta-w_j\|)\Phi_\epsilon'(\|\theta - w_j\|)| \geq 1/\poly(k)(d/\epsilon)^{-4d}$, which contradicts $\boldsymbol{(a,\theta)}$ in $\mathcal{M}_{G,(d/\epsilon)^{-O(d)}}$ since $\|\nabla_\theta G_1 \| \geq 1/\poly(k)(d/\epsilon)^{-4d}$. 

Therefore, it must be that while running SGD, there exists some iterate $x_l$ such that $x_l$ in within a $(d/\epsilon)^{-O(d)}$ neighborhood of some $w_j$. By our observation above, we see that $-\nabla_\theta G_1(x_l) = 2b_j^2\Phi_\epsilon(\|x_l-w_j\|)\Phi_\epsilon'(\|x_l - w_j\|) \frac{x_l - w_j}{\|x_l - w_j\|}+ (d/\epsilon)^{-O(d)}$. Note that $\Phi_\epsilon$ and $\Phi_\epsilon'$ differ in sign, so $x_{l}$ moves in the direction of $w_j$ with negligible remainder terms. Since we are taking step-sizes of $(d/\epsilon)^{-O(d)}$, we conclude that $x_T$ remains also in a $(d/\epsilon)^{-O(d)}$ neighborhood of $w_j$. 
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "icmlpaper2017.tex"
%%% End:

\section{Realizable Potentials with Convergence Guarantees}

{\color{red} Under some assumptions, we
will show that gradient descent can learn at least one of the $w_i's$
of the target network for certain activation functions. The algorithm
will try to learn a guess
$\widetilde{f}(x_j) = \sum_{i=1}^k a_i \sigma(x_j,\theta_i)$ for $f$
and then running gradient descent over the parameters $a_i, \theta_i$
will move them to $b_i, w_i$. We will prove that at convergence, at
least one $\theta_i$ is equal to (or close to) some $w_j$. Note that
we may end up with a many to one mapping of the learned hidden weights
to the true hidden weights, instead of a bijection.


% Next, we provide a partial theoretical justification for this phenomena with simplifying assumptions.
}
 
In this section, we derive convergence guarantees for a class of realizable potentials that closely approximate $\lambda$-harmonic potentials. First, we construct realizable potentials with corresponding activation functions that are approximately $\lambda$-harmonic, specifically they are $\lambda$-harmonic outside of a small neighborhood around the center. We will defer the details of the technical construction to the appendix. Then, we reason similarly about the Laplacian of our loss function to derive our convergence theorem. To make sure that $\|a\|$ remains controlled throughout the optimization process, we add a quadratic regularization term to $L$ and instead run our optimization procedure on $G = L + \|a\|^2$.

Our optimization procedure is a slightly altered version of gradient descent, where second-order methods are used when the gradient is too small and progress is slow. By using a second-order gradient descent (Algorithm~\ref{SecondGD}), we are able to converge to points with small gradient and small negative curvature. Namely, in $\poly(1/\epsilon)$ iterations, we should have reached a point in $\mathcal{M}_{G, \epsilon}$, where 
%
%
\[\mathcal{M}_{G, \epsilon} = \left\{x\in \mathcal{M} \Big| \|\nabla G(x)\|
  \leq \epsilon \text{ and } \lambda_{min}(\nabla^2 G(x)) \geq
  -\epsilon\right\}\]
%
Then, we show that if $(\boldsymbol{a,\theta})$ is in $\mathcal{M}_{G, \epsilon}$ for $\epsilon$ small, then $\theta_i$ is close to $w_j$ for some $j$. Finally, we show how to initialize $(\boldsymbol{a^{(0)},\theta^{(0)}})$ and run SGD to converge to $\mathcal{M}_{G,\epsilon}$, proving our following theorem.

The use of second-order methods is not limiting since noisy gradient descent algorithms can descent along negative curvature directions. Therefore, stochastic gradient descent should also converge to $M_{G,\epsilon}$ \cite{GeHJY15}, although we lack some regularity conditions. Furthermore, a more controlled perturbed gradient descent \cite{Jin0NKJ17} can be applied in our setting to reach $M_{G,\epsilon}$ but requires more in-depth analysis.
%
\begin{algorithm}[hb]
 \caption{$x = HD(L,x_0, T,\alpha$)}
   \label{HD}
\begin{algorithmic}
   \STATE {\bfseries Input:} $L: \mathcal{M} \to \R$; $x_0 \in \mathcal{M}$; $T\in \N$; $\alpha, \gamma \in \R$
   \STATE Initialize $x = x_0$
   \FOR{$i=1$ {\bfseries to} $T$}
   \STATE Find unit eigenvector $v_{min}$ corresponding to $\lambda_{min}(\nabla^2 f(x))$ 
   \STATE $\beta = -\alpha \lambda_{min}(\nabla^2 f(x)) sgn(\nabla f(x)^Tv_{min}) $
    \STATE $x = x + \beta v_{min}$
   \ENDFOR
\end{algorithmic}
\end{algorithm}
%
\begin{algorithm}[hb]
 \caption{$x = SecondGD(L, x_0, T,\alpha, \eta, \gamma)$}
   \label{SecondGD}
\begin{algorithmic}
   \STATE {\bfseries Input:} $L:\mathcal{M} \to \R$; $x_0 \in \mathcal{M}$; $T\in \N$; $\alpha \in \R$; $\epsilon\in\R$
   \vspace{.1in}
   \FOR{$i=1$ {\bfseries to} $T$}
   \IF{$\|\nabla L(x_{i-1})\| \geq  \eta$}
   \STATE $x_{i} = GD(L, x_{i-1}, 1, \alpha)$
   \ELSE 
   \STATE Find unit eigenvector $v_{min}$ corresponding to $\lambda_{min}(\nabla^2 f(x_{i-1}))$ 
    \IF{$\lambda_{min}(\nabla^2 f(x_{i-1})) \leq - \gamma$}
    \STATE $x_i = HD(L, x_{i-1}, 1, \alpha)$
    \ELSE 
     \STATE {\bf return} $x_{i-1}$
    \ENDIF
   \ENDIF
   \IF{$ L(x_i) \geq L(x_{i-1}) - min(\alpha\eta^2/2, \alpha^2 \gamma^3/2) $}
     \STATE {\bf return} $x_{i-1}$
   \ENDIF
   \ENDFOR
   \end{algorithmic}
\end{algorithm}
%
\begin{theorem}\label{almostHarmSGD}
  Let $\mathcal{M} = \R^{d}$ for $d \equiv 3 \mod 4$. For all $\epsilon \in (0,1),$ there exists an activation $\sigma$ such that if $w_1,...,w_k \in \R^d$ with $w_i$ randomly chosen from $w_i \sim  \mathcal{N}({\bf 0}, O(d\log k){\bf I_{d\times d}})$ and $b_1,...,b_k$ be randomly chosen at uniform from $[-1,1]$, then with high probability, we can choose an initial point $(\boldsymbol{a^{(0)}, \theta^{(0)}})$ such that after running SecondGD (Algorithm \ref{SecondGD}) on the regularized objective $G(\boldsymbol{a,\theta})$ for at most $\poly(1/\lambda)d^{\sqrt{\lambda}}(d/\epsilon)^{O(d)}$ iterations, there exists an $i, j$ such that $\|\theta_i - w_j\| <  \epsilon$.
\end{theorem}


We start by stating a lemma concerning the construction of an approximately
$\lambda$-harmonic function on $\R^d.$ The construction is given in
Section~\ref{realizable} and is quite technical.
%
%
\begin{restatable}{lemma}{AlmostHarmReal}\label{almostHarmReal}
Let $\mathcal{M} = \R^d$ for $d \equiv 3 \mod 4$. Then, for any $1 > \epsilon > 0$, we can construct a radial activation $\sigma_\epsilon(r)$ with corresponding normalized radial potential $\Phi_\epsilon(r)$ that is $\lambda$-harmonic when $r \geq \epsilon$.

Furthermore, we have ${\Phi_\epsilon}^{(d-1)}(r) \geq 0$ for all $r  > 0$ and ${\Phi_\epsilon}^{(k)}(r) \geq 0$ and ${\Phi_\epsilon}^{(k+1)}(r)\leq 0$ for all $r > 0$ and $d - 3 \geq k \geq 0 $ even.

Lastly, $|{\Phi}_\epsilon^{(k)}(r)| \leq 3(2d + \sqrt{\lambda})^{2d} \epsilon^{-2d}e^{\sqrt{\lambda}}$ for all $0 \leq k \leq d-1$. And for $r \geq \epsilon$, $e^{-\sqrt{\lambda}r}r^{2-d}(2d+\sqrt{\lambda})^{-2d}\epsilon^{2d}/3\leq {\Phi}_\epsilon(r) \leq (1+r\sqrt{\lambda})^de^{\sqrt{\lambda}(1-r)}(r)^{2-d}$. Also for $r \geq \epsilon$, $ e^{-\sqrt{\lambda}r}r^{1-d}(2d+\sqrt{\lambda})^{-2d}\epsilon^{2d}/3 \leq |{\Phi}_\epsilon'(r)| \leq (d+\sqrt{\lambda}r)(1+ r\sqrt{\lambda})^de^{\sqrt{\lambda}(1- r)} r^{1-d}$
\end{restatable}
%
%
The proof is similar to Theorem \ref{EigStrict}.
%
\begin{restatable}{lemma}{AlmostHarmConv}\label{almostHarmConv}
Let $\mathcal{M} = \R^d$ for $d \equiv 3 \mod 4$ and let $L$ be as in \eqref{errLoss} corresponding to the activation function $\sigma_\epsilon$ given by Lemma~\ref{almostHarmReal}. For any $1 > \epsilon > 0$ and $2d/\lambda > \delta > 0$, we can construct $\sigma$ such that if $\boldsymbol{(a,\theta)} \in \mathcal{M}_{G,\delta}$, then for all $i$, either 1) there exists $j$ such that $\|\theta_i - w_j\| < k\epsilon$ or 2) $a_i^2 < 2kd\delta/\lambda$.
\end{restatable}
%
The charges are big if we made progress
%
\begin{restatable}{lemma}{AlmostHarmRes}\label{almostHarmRes}
  Assume the conditions of Lemma~\ref{almostHarmConv}. If
$\sqrt{G({\bf a, \boldsymbol{\theta}})} \leq \sqrt{G(\boldsymbol{0,0})} - \delta$
  and $(\boldsymbol{a,\theta}) \in \mathcal{M}_{G,\lambda \delta^2/(k^3d)}$,
  then there exists some $i, j$ such that $\|\theta_i - w_j\| <k\epsilon$.
\end{restatable}
 %
 Next, we guarantee progress.
 %
 \begin{restatable}{lemma}{AlmostHarmInitialize}[Initialization]\label{almostHarmInitialize}
Assume the conditions of Theorem~\ref{almostHarmSGD} and Lemma~\ref{almostHarmConv}. With high probability, we can initialize $\boldsymbol{(a^{(0)},\theta^{(0)})}$ such that $\sqrt{G({\bf a^{(0)}}, \boldsymbol{\theta^{(0)}})} \leq \sqrt{G(\boldsymbol{0,0})} -\delta$ with $\delta = \frac{1}{1+\beta}d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{ - O(d)}$.
 \end{restatable}
 %
\begin{proof}[Proof of Theorem \ref{almostHarmSGD}]
Let our potential $\Phi_{\epsilon/k}$ be the one as constructed in Lemma~\ref{almostHarmReal} that is $\lambda$-harmonic for all $r \geq \epsilon/k$ and as always, $k = \poly(d)$.  First, by Lemma~\ref{almostHarmInitialize},  we can initialize $\boldsymbol{(a^{(0)},\theta^{(0)})}$ such that $G(\boldsymbol{a^{(0)},\theta^{(0)}}) \leq  G({\bf 0,0}) - 2\delta \sqrt{G({\bf 0,0})}$ for $\delta = d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{-O(d)}$. If we set $\alpha = \poly(1/\lambda)d^{\sqrt{\lambda}}(d/\epsilon)^{-O(d)}$ and $\eta = \gamma = \lambda \delta^2/(k^3d)$,  then running Algorithm~\ref{SecondGD} will terminate and return some $(\boldsymbol{a,\theta})$ in at most $\poly(1/\lambda)d^{\sqrt{\lambda}}(d/\epsilon)^{O(d)}$ iterations. This is because our algorithm ensures that our objective function decreases by at least $\min(\alpha \eta^2/2, \alpha^2\gamma^3/2)$ at each iteration and $G({\bf 0, 0})$ is bounded by $O(k)$ and $G \geq 0$ is non-negative.

Let $\boldsymbol{\theta} = (\theta_1,...\theta_k)$. If there exists $\theta_i, w_j$ such that $\|\theta_i - w_j\| < \epsilon$, then we are done. Otherwise, we claim that $(\boldsymbol{a,\theta}) \in \mathcal{M}_{G,\lambda \delta^2/(k^3d)}$. For the sake of contradiction, assume otherwise. By our algorithm termination conditions, if $(\boldsymbol{a,\theta}) \not \in \mathcal{M}_{G,\lambda \delta^2/(k^3d)}$, then it must be that after one step of gradient or Hessian descent from $(\boldsymbol{a,\theta})$, we reach some $(\boldsymbol{a',\theta'})$ and $G(\boldsymbol{a',\theta'}) \geq G(\boldsymbol{a,\theta}) - \min(\alpha\eta^2/2,\alpha^2\gamma^3/2)$.

Now, Lemma~\ref{almostHarmReal} ensures all first three derivatives of $\Phi$ are bounded by $(d/\epsilon)^{2d}$, except at $w_1,...,w_k$. Furthermore, since there does not exists $\theta_i, w_j$ such that $\|\theta_i - w_j\| <\epsilon$, $G$ is three-times continuously differentiable within a $\alpha (d/\epsilon)^{2d} = (d/\epsilon)^{-O(d)}$ neighborhood of $\boldsymbol{\theta}$. Therefore, by Lemma~\ref{GradDecrease} and ~\ref{HessianDecrease}, we must have  we know that $G(\boldsymbol{a',\theta'}) \leq G(\boldsymbol{a,\theta}) - \min(\alpha\eta^2/2,\alpha^2\gamma^3/2)$. a contradiction. Lastly, since our algorithm maintains that our objective function is decreasing, so $G(\boldsymbol{a,\theta}) \leq G({\bf 0,0}) - 2\delta \sqrt{G({\bf 0,0})}$. Finally, we conclude by Theorem \ref{almostHarmRes}.
\end{proof}

\subsection{Node-by-Node Analysis}
We cannot easily analyze the convergence of GD to the global minima when all $\theta_i$ are simultaneously moving since the pairwise interaction terms between the $\theta_i$ present complications. To derive a tighter control on $(\boldsymbol{a,\theta})$, we run a greedy
node-wise SGD algorithm to learn the hidden weights, i.e. we run a
full SGD algorithm with respect to $(a_i,\theta_i)$ sequentially. The
main idea is that after running SGD with respect to $\theta_1$,
$\theta_1$ should be close to some $w_j$ for some $j$. Then, we can
carefully induct and show that $\theta_2$ must be some other $w_k$ for
$k\neq j$ and so on.

%
\begin{algorithm}[tb]
 \caption{Node-wise Gradient Descent Algorithm with Output Weights Optimization}
   \label{NodeGDOpt}
\begin{algorithmic}
  \STATE {\bfseries Input:}
  $(\boldsymbol{a,\theta}) = (a_1,...,a_k,\theta_1,...,\theta_k), a_i
  \in\R, \theta_i\in\mathcal{M}$;
  $T\in \N$; $\widehat{L}$; $\alpha\in \R$; $\delta \in \R$;
  $\gamma \in R$; \vspace{0.1in} \STATE{\bf Initialize} $\boldsymbol{(a,\theta) = (0,0)}$
  \FOR{$i=1$ {\bfseries to} $k$} 
  \STATE $(a_i, \theta_i) = NoisyGD \left(\widehat{L}_{a_i, \theta_i},(a_i,\theta_i),T, \alpha,\delta \right)$
   \STATE    $(a_1,...,a_i) =  GD \left(\widehat{L}_{a_1,..,a_i},
     (a_1,..,a_i), T , \alpha \right)$\;
   \ENDFOR
   \STATE {\bf return} $a = (a_1,...,a_k), \theta = (\theta_1,..., \theta_k)$
   \end{algorithmic}
\end{algorithm}

To apply our node-wise guarantees, we would first need to first tighten the convergence properties of SGD in the case where we have only 1 variable charge, say $\theta_1$. Let $L_1(a_1,\theta_1)$ be the objective $L$ restricted to $a_1,\theta_1$ being variable, and $a_2,...,a_k = 0$ are fixed. Our better control on the movements of $\theta_1$ allows us to remove our regularization. While our previous guarantees before allow us to reach a $\epsilon$-neighborhood of $w_j$ when running SGD on $L_1$, we actually would like to reach a $(d/\epsilon)^{-O(d)}$-neighborhood of $w_j$. This is accomplished by reasoning about the first derivatives of our potential, mainly by showing that in an $\epsilon$-neighborhood of $w_j$, descending along the gradient always brings $\theta_1$ closer to $w_j$.
%
\begin{lemma}\label{nodewiseSGD}
Let $\mathcal{M} = \R^{d}$. For all $\epsilon \in (0,1),$ there exists an activation $\sigma$ such that if $w_1,...,w_k \in \R^d$ with $w_i$ randomly chosen from $w_i \sim  \mathcal{N}({\bf 0}, O(d\log k){\bf I_{d\times d}})$ and $b_1,...,b_k$ be randomly chosen at uniform from $[-1,1]$, then with high probability, we can choose an initial point $(a^{(0)}, \theta^{(0)})$ such that after running SGD (Algorithm \ref{SGD}) on the restricted regularized objective $L_1(a,\theta)$ for at most $\poly(1/\beta,1/\lambda)d^{\sqrt{\lambda}}(d/\epsilon)^{O(d)}$ iterations, there exists some $w_j$ such that $\|\theta - w_j\| < O(d/\epsilon)^{-O(d)}$ and $|a + b_j| < O(d/\epsilon)^{-O(d)}$.
\end{lemma}


\begin{proof}
First, by Lemma~\ref{almostHarmInitialize},  we can initialize ${(a^{(0)},\theta^{(0)})}$ such that $L_1({a^{(0)},\theta^{(0)}}) \leq  L_1({\bf 0,0}) - 2\delta \sqrt{L_1({\bf 0,0})}$ for $\delta = d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{-O(d)}$. Next, Lemma~\ref{almostHarmReal} ensures all first three derivatives of $\Phi$ are bounded by $O((d/\epsilon)^{2d})$, except at $w_1,...,w_k$. Now, consider running noisy GD to generate a sequence $x_1 = (a^{(1)},\theta^{(1)}),...,x_T = (a^{(T)},\theta^{(T)})$ with step-sizes of $(d/\epsilon)^{-O(d)}$ so that each gradient step moves at most a distance of $(d/\epsilon)^{-O(d)}$. 

If, for all $i$, the iterates $x_i \not \in \mathcal{M}_{G,\beta\lambda\delta^2/2k^2}$ and $\theta_i$ is not within a $(d/\epsilon)^{-O(d)}$ neighborhood of $w_1,...,w_k$, then we derive a contradiction by looking at the decrease in the expected objective function. Since our derivatives are bounded outside of a neighborhood of $w_1,...,w_k$, by Lemma~\ref{GradDecrease} and ~\ref{HessianDecrease}, we know that our expected objective function is decreasing by $(d/\epsilon)^{-O(d)}$ in at most $(d/\epsilon)^{O(d)}$ iterations, if we set $\alpha = \poly(1/\beta,1/\lambda)d^{\sqrt{\lambda}}(d/\epsilon)^{O(d)}$. Since $G({\bf 0, 0})$ is bounded by $O(k)$ and $G \geq 0$, then we derive a contradiction since the expected decrease is bounded by $O(k)$. 

Now, there must exist $x_i \in \mathcal{M}_{G,\beta\lambda\delta^2/2k^2}$ or $x_i$ is within a $(d/\epsilon)^{-O(d)}$ neighborhood of $w_1,...,w_k$ for some $i$. Assume the former holds and the latter does not. First, by Azuma's inequality, in $(d/\epsilon)^{O(d)}$ iterations, we can guarantee that $L_1(\boldsymbol{a,\theta}) \leq L_1({a^{(0)},\theta^{(0)}}) + (d/\epsilon)^{O(d)} \leq L_1({\bf 0,0}) - \delta \sqrt{L_1({\bf 0,0})}$ with high probability for $\delta = d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{-O(d)}$. Finally, we conclude by Theorem \ref{almostHarmRes} that there exists $w_j$ such that $\|\theta - w_j\| < \epsilon$.

Now, our gradient with respect to $\theta$ is
%
\begin{align*}
\nabla_\theta L_1 &= 2ab_j \Phi_\epsilon'(\|\theta - w_j\|) \frac{\theta - w_j}{\|\theta - w_j\|}+ 2\sum_{i\neq j} ab_i\Phi_\epsilon'(\|\theta - w_i\|) \frac{\theta - w_i}{\|\theta - w_i\|}
\end{align*}
%

By our construction, since $\|\theta - w_j\| \leq \epsilon$, we may lower bound $|\Phi_\epsilon'(\|\theta - w_j\|)| \geq e^{-\sqrt{\lambda}\epsilon}\epsilon^{1-d}(2d+\sqrt{\lambda})^{-2d}\epsilon^{2d}/3 \geq O((d/\epsilon)^{-2d})$. On the other hand, for all $i \neq j$, we note that with high probability $\|w_i - w_j\| \geq \Omega(d \log k)$. Therefore, we may upper bound $|\Phi_{\epsilon}(\|\theta - w_i\|)| \leq 1/\poly(k)(d/\epsilon)^{-O(d)}$. Finally, we note that all $|b_i | \geq 1/\poly(k)$ with high probability. 

Together, we conclude that $\nabla_\theta L_1 = 2ab_j \Phi_\epsilon'(\|\theta - w_j\|) \frac{\theta - w_j}{\|\theta - w_j\|} + 2a\eta$, where $\|\eta\| \leq (d/\epsilon)^{-O(d)} b_j \Phi_\epsilon'(\|\theta - w_j\|) \frac{\theta - w_j}{\|\theta - w_j\|}$. 

Similarly, since $(a,\theta) \in \mathcal{M}_{L_1, (d/\epsilon)^{-O(d)}}$, we know that
%
\begin{align*}
   \abs{\pd{L_1}{a}} & = \lvert 2a  + 2b_j \Phi_\epsilon(\|\theta - w_j\|) + 2\sum_{i \neq j} b_i \Phi_\epsilon(\|\theta - w_i\|) \rvert \leq (d/\epsilon)^{-O(d)}
\end{align*}

By a similar argument as on the derivative, we see that $a = -2b_j \Phi_\epsilon(\|\theta - w_j\|) + O(d/\epsilon)^{-O(d)}$ and $|b_j \Phi_\epsilon(\|\theta - w_j\|) | \geq (d/\epsilon)^{-2d}$. By combining our observations, we realize that since noisy GD is moving in the direction of $-\nabla_\theta G_1$, it is moving $\theta$ closer to $w_j$ since 
%
\begin{align*}
\nabla_\theta L_1 &=  2ab_j \Phi_\epsilon'(\|\theta - w_j\|) \frac{\theta - w_j}{\|\theta - w_j\|} + (d/\epsilon)^{-O(d)} \\
&= -b_j^2\Phi_\epsilon(\|\theta-w_j\|)\Phi_\epsilon'(\|\theta - w_j\|) \frac{\theta - w_j}{\|\theta - w_j\|} + (d/\epsilon)^{-O(d)} 
\end{align*}

However, we see that with high probability, $|b_j^2\Phi_\epsilon(\|\theta-w_j\|)\Phi_\epsilon'(\|\theta - w_j\|)| \geq 1/\poly(k)(d/\epsilon)^{-4d}$, which contradicts $\boldsymbol{(a,\theta)}$ in $\mathcal{M}_{G,(d/\epsilon)^{-O(d)}}$ since $\|\nabla_\theta G_1 \| \geq 1/\poly(k)(d/\epsilon)^{-4d}$. 

Therefore, it must be that while running noisy GD, there exists some iterate $x_l$ such that $x_l$ in within a $(d/\epsilon)^{-O(d)}$ neighborhood of some $w_j$. By our observation above, we see that $-\nabla_\theta G_1(x_l) = 2b_j^2\Phi_\epsilon(\|x_l-w_j\|)\Phi_\epsilon'(\|x_l - w_j\|) \frac{x_l - w_j}{\|x_l - w_j\|}+ (d/\epsilon)^{-O(d)}$. Note that $\Phi_\epsilon$ and $\Phi_\epsilon'$ differ in sign, so $x_{l}$ moves in the direction of $w_j$ with negligible remainder terms. Since we are taking step-sizes of $(d/\epsilon)^{-O(d)}$, we conclude that $x_T$ remains also in a $(d/\epsilon)^{-O(d)}$ neighborhood of $w_j$. 

Finally, we see that the charges also converge since $a = -2b_j \Phi_\epsilon(\|\theta - w_j\|) + O(d/\epsilon)^{-O(d)}$ and $\|\theta - w_j\| = (d/\epsilon)^{-O(d)}$. By noting that $\Phi_\epsilon(0) = 1$ and $\Phi_\epsilon$ is $(d/\epsilon)^{2d}$-Lipschitz, we conclude. 
\end{proof}

\begin{theorem}\label{nodewiseSGD}
Let $\mathcal{M} = \R^{d}$ and let $L$ be as in \ref{errLoss} with some activation $\sigma$. For all $\epsilon \in (0,1),$ there exists an activation $\sigma$ such that if $w_1,...,w_k \in \R^d$ with $w_i$ randomly chosen from $w_i \sim  \mathcal{N}({\bf 0}, O(d\log k){\bf I_{d\times d}})$ and $b_1,...,b_k$ be randomly chosen at uniform from $[-1,1]$, then with high probability, after running nodewise SGD (Algorithm \ref{NodeGDOpt}) on the objective $L$ for at most $\poly(1/\beta,1/\lambda)d^{\sqrt{\lambda}}(d/\epsilon)^{O(d)}$ iterations, $\boldsymbol{(a,\theta)}$ is in a $(d/\epsilon)^{-O(d)}$ neighborhood of the global minima.
\end{theorem}

\begin{proof}
Let $(a_i, \theta_i)$ be the $i$-th node that is initialized and applied noisy gradient descent onto. We want to show that the nodes $(a_i, \theta_i)$ will converge, in a node-wise fashion, to some permutation of $\{(b_1,w_1),...,(b_k,w_k)\}$. By Lemma~\ref{nodewiseSGD}, we know that with high probability $(a_1,\theta_1)$ will converge to some $(d/\epsilon)^{-O(d)}$ neighborhood of $(b_{\pi(1)}, w_{\pi(1)})$ for some function $\pi: [k] \to [k]$. Now, since [condition holds], by Lemma~\ref{almostHarmInitialize}, we can initialize ${(a_2^{(0)},\theta_2^{(0)})}$ such that $L_2({a_2^{(0)},\theta_2^{(0)}}) \leq  L_2({\bf 0,0}) - 2\delta \sqrt{L_2({\bf 0,0})}$ for $\delta = d^{-O(d\sqrt{\lambda})}(d/\epsilon)^{-O(d)}$. Then, by Lemma~\ref{nodewiseSGD}, we know that if $x_i = (a_2^{(i)}, \theta_2^{(i)}$ are the noisy GD iterates, then there exists a minimal $l$ such that $x_l$ is in a $(d/\epsilon)^{-O(d)}$ neighborhood of $w_j$ for some $j$. [Needs to be argued further] By Azuma's inequality, $L_2({a_2^{(l)},\theta_2^{(l)}}) \leq  L_2({\bf 0,0}) - \delta \sqrt{L_2({\bf 0,0})}$, with high probability. 

Now, we claim that $x_l$ is not in a $(d/\epsilon)^{-O(d)}$ neighborhood of $(b_{\pi(1)}, w_{\pi(1)})$. Assume otherwise. First, we see that our decrease in objective function implies that $L_{a_1,\theta_1,a_2,\theta_2}(a_1,\theta_1, a_2^{(l)},\theta_2^{(l)}) \leq L_{a_1,\theta_1,a_2,\theta_2}(a_1,\theta_1, {\bf 0,0}) - \delta \sqrt{L_2({\bf 0,0})}$. However, notice that our assumption implies that $a_2^{(l)},\theta_2^{(l)}$ is in a $(d/\epsilon)^{-O(d)}$ neighborhood of $a_1,\theta_1$. By the smoothness of $L$, we see that $L_{a_1,\theta_1,a_2,\theta_2}(a_1,\theta_1, a_2^{(l)},\theta_2^{(l)}) \geq L_{a_1,\theta_1,a_2,\theta_2}(a_1,\theta_1, a_2^{(l)},\theta_1) - (d/\epsilon)^{-O(d)} = L_{a_1,\theta_1,a_2,\theta_2}(a_1 + a_2^{(l)},\theta_1, {\bf 0, 0}) - (d/\epsilon)^{-O(d)} $. Together, we know that  $L_{a_1,\theta_1,a_2,\theta_2}(a_1+a_2^{(l)},\theta_1,{\bf 0, 0}) \leq L_{a_1,\theta_1,a_2,\theta_2}(a_1,\theta_1, {\bf 0,0}) - (d/\epsilon)^{-O(d)}$

However, by Theorem \ref{quadConverge}, our gradient descent guarantees that $L_{a_1,\theta_1,a_2,\theta_2}(a_1,\theta_1, {\bf 0,0}) - L_{a_1,\theta_1,a_2,\theta_2}(a_1 + a_2^{(l)},\theta_1, {\bf 0, 0}) \leq O(1/T)$, where $T$ is the number of iterations of gradient descent. Since $T = (d/\epsilon)^{-O(d)}$ is large enough, we derive a contradiction. Therefore, our claim is done and by induction, $\pi$ is a permutation. Now, our theorem follows. 
\end{proof}

%
%%
%\begin{algorithm}[hb]
% \caption{$x = NoisyGD(\widehat{L}, x_0, T,\alpha,\epsilon)$}
%   \label{SGD}
%\begin{algorithmic}
%   \STATE {\bfseries Input:} $\widehat{L}:\mathcal{M} \to \R$; $x_0 \in \mathcal{M}$; $T\in \N$; $\alpha \in \R$; $\epsilon\in\R$
%   \vspace{.1in}
%   \STATE {\bf Initialize} $x = x_0$
%   \WHILE{{\it true}}
%   \STATE $x_1 = x$
%   \FOR{$i=1$ {\bfseries to} $T$}
%   \STATE Sample $\omega$ uniformly on unit sphere.
%   \STATE $x_{i+1} = x_i - \alpha(\nabla \widehat{L} (x_i)+\alpha\omega)$ 
%%\STATE $x = \Pi_\mathcal{M} x$
%   \ENDFOR
%    \IF{$\widehat{L}(x_i) - \widehat{L}(x) < \alpha^2$ for all $i$}
%   \STATE {\bf return} $x$
%   \ELSE 
%   \STATE Find $i$ such that $\widehat{L}(x_i) - \widehat{L}(x) \geq \alpha^2$ 
%   \STATE $x = x_i$
%   \ENDIF
%   \ENDWHILE
%\end{algorithmic}
%\end{algorithm}
%%

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "icmlpaper2017.tex"
%%% End:
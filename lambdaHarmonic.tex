
\section{Realizable Potentials with Convergence Guarantees}

In this section, we derive convergence guarantees for a class of realizable potentials that closely approximate $\lambda$-harmonic potentials. First, we construct realizable potentials with corresponding activation functions that are approximately $\lambda$-harmonic, specifically they are $\lambda$-harmonic outside of a small neighborhood around the center. We will defer the details of the technical construction to the appendix. Then, we reason similarly about the Laplacian of our loss function to derive the following theorem.

\begin{theorem}\label{almostHarmSGD}
  Let $\mathcal{M} = \R^{d}$. For all $\epsilon \in (0,1),$ there exists an activation $\sigma$ such that if $w_1,...,w_k \in \R^d$ with $\|w_i\|\leq d$ and $b_1,...,b_k$ be randomly chosen at uniform from $[-1,1]$, then with high probability, we can choose an initial point $(\boldsymbol{a^{(0)}, \theta^{(0)}})$ and after running SGD (Algorithm \ref{SGD}) on the regularized objective
  $G(\boldsymbol{a,\theta})$ for at most $(d/\epsilon)^{O(d)}$ iterations, we have either 
  
  1) $G(\boldsymbol{a,\theta}) \leq \epsilon$ or 
  
  2) there exists an $i, j$ such that $\|\theta_i - w_j\| <  \epsilon$ 
\end{theorem}


To make sure that $\|a\|$ remains controlled throughout the optimization process, we add a quadratic regularization term to $L$ and instead run SGD on $G = L + \beta\|a\|^2$. By reusing techniques in \cite{GeHJY15}, we show that SGD (with noise) can avoid all points that are have a negative curvature of at least $\epsilon$ in $\poly(1/\epsilon)$ iterations. Intuitively, this means that SGD will converge to points with small gradient and small negative curvature, converging to a point in $\mathcal{M}_{G, \epsilon}$, where 
%
%
\[\mathcal{M}_{G, \epsilon} = \left\{x\in \mathcal{M} \Big| \|\nabla G(x)\|
  \leq \epsilon \text{ and } \lambda_{min}(\nabla^2 G(x)) \geq
  -\epsilon\right\}\]
%
Then, we show that if $(\boldsymbol{a,\theta})$ is in $\mathcal{M}_{G, \epsilon}$ for $\epsilon$ small, then $\theta_i$ is close to $w_j$ for some $j$. Finally, we show how to initialize $(\boldsymbol{a^{(0)},\theta^{(0)}})$ and run SGD to converge to $\mathcal{M}_{G,\epsilon}$, proving our main theorem.

We start by stating a lemma concerning the construction of an approximately
$\lambda$-harmonic function on $\R^d.$ The construction is given in
Section~\ref{realizable} and is quite technical.
%
%
\begin{restatable}{lemma}{AlmostHarmReal}\label{almostHarmReal}
Let $\mathcal{M} = \R^d$ for $d \equiv 3 \mod 4$. Then, for any $1 > \epsilon > 0$, we can construct a realizable radial potential $\Phi_\epsilon(r)$ that is $\lambda$-harmonic when $r \geq \epsilon$.

Furthermore, we have ${\Phi_\epsilon}^{(d-1)}(r) \geq 0$ for all $r  > 0$ and ${\Phi_\epsilon}^{(k)}(r) \geq 0$ and ${\Phi_\epsilon}^{(k+1)}(r)\leq 0$ for all $r > 0$ and $d - 3 \geq k \geq 0 $ even.

Lastly, $|\Phi_\epsilon^{(k)}(r)| \leq 3(2d + \epsilon \sqrt{\lambda})^{2d}\epsilon^{-2d}  $ for all $0 \leq k \leq d-1$.
\end{restatable}
%
%
%
\begin{lemma}\label{almostHarmConv}
Let $\mathcal{M} = \R^d$ for $d \equiv 3 \mod 4$ and let $L$ be as in \eqref{errLoss} corresponding to some activation function $\sigma$. For any $\epsilon, \delta > 0$, we can construct $\sigma$ such that if $\boldsymbol{(a,\theta)} \in \mathcal{M}_{G,\poly(\delta,1/d)}$, then for all $i$, either 1) there exists $j$ such that $\|\theta_i - w_j\| < \epsilon$ or 2) $a_i^2 < \delta/(\beta\lambda)$
\end{lemma}
%
\begin{proof}
 The proof is similar to Theorem \ref{EigStrict}. Let $\Phi_\lambda$ be the realizable potential in \ref{almostHarmReal} such that $\Phi_\lambda(r)$ is $\lambda$-harmonic when $r \geq \epsilon/k$. Note that $\Phi_\lambda(0) = 1$ is normalized. And let $\boldsymbol{(a,\theta)} \in \mathcal{M}_{G,\poly(\epsilon,1/d)}$. 
 
WLOG, consider $\theta_1$ and a initial set $S_0 = \{ \theta_1\}$ containing it. For a finite set of points $S$ and a point $x$, define $d(x,S) = \min_{y \in S} \| x - y\|$. Then, we consider the following set growing process. If there exists $\theta_i, w_i \not \in S_j$ such that $d(\theta_i, S_j) < \epsilon/k$ or $d(w_i, S_j) < \epsilon/k$, add $\theta_i, w_i$ to $S_j$ to form $S_{j+1}$. Otherwise, we stop the process. We grow $S_0$ to until the process terminates and we have the grown set $S$.

If there is some $w_j \in S$, then it must be the case that there exists ${j_1},\cdots {j_q}$ such that $\|\theta_1 - \theta_{j_1} \| < \epsilon/k$ and
$\|\theta_{j_{i}} - \theta_{j_{i+1}}\| < \epsilon/k$, and
$\|\theta_{j_q}- w_j\| <\epsilon/k$ for some $w_j$. So, there exists $j$, such that $\|\theta_1 - w_j\| < \epsilon$. 

Otherwise, notice that for each $\theta_i \in S$, $\|w_j - \theta_i\|\geq \epsilon/k$ for all $j$, and $\|\theta_i - \theta_j\| \leq \epsilon/k$ for all $\theta_j\not \in S$. WLOG, let $S = \{\theta_1,\dots,\theta_l\}$. 
  
We consider changing all
$\theta_1, \ldots, \theta_{l}$ by the same $v$ and define 
%
\[H({\bf a}, v) = G({\bf a},\theta_1+v,...,\theta_l+v, \theta_{l+1}
\ldots, \theta_k).\]

The optimality conditions on ${\bf a}$ are 
\begin{align*}
   \abs{\pd{H}{a_i}} & = \lvert (2+2\beta)a_i  + 2\sum_{j\neq i} a_j \Phi(\theta_i,\theta_j) + 2\sum_{j=1}^k b_j \Phi(\theta_i,w_j) \rvert \\
& \leq \poly(\delta,1/d)
\end{align*}
%
Next, since $\Phi_\lambda(r)$ is $\lambda$-harmonic for $r \geq \epsilon/k$, we may calculate the Laplacian of $H$ as
%
\begin{align*}
\Delta_v H & = \sum_{i=1}^l \lambda \left(2\sum_{j=1}^k a_ib_j
  \Phi(\theta_i, w_j) + 2\sum_{j=l+1}^k a_ia_j
  \Phi(\theta_i, \theta_j)\right) \\
& = \sum_{i=1}^l \lambda \left(-2\beta a_i^2 -2a_i^2 - 2
  \sum_{j = 1, j\neq i}^l  a_ia_j \Phi(\theta_i,\theta_j)\right) \\
  & \qquad \qquad  + \poly(\delta,1/d) \sum_{i=1}^l \lambda a_i^2 \\
&= -2\lambda\expt\left[\left( \sum_{i=1}^l a_i \sigma(\theta_i,X)\right)^2\right] -2\beta\lambda \sum_{i=1}^l a_i^2\\
& \qquad \qquad + \poly(\delta,1/d) \sum_{i=1}^l \lambda a_i^2 \\
\end{align*} 
%
The second line follows from our optimality conditions and the third line follows from completing the square. Since $\boldsymbol{(a,\theta)} \in \mathcal{M}_{G,\poly(\delta,1/d)}$, we have $\Delta_v H \geq - \poly(\delta,1/d)$. By choosing $\beta \geq \poly(\delta,1/d)$, we see that we must satisfy $\sum_{i=1}^l a_i^2 \leq \poly(\delta,1/d)/(\beta\lambda)$. Therefore, $a_i^2 \leq \delta/(\beta\lambda)$.

\end{proof}

\begin{lemma}\label{almostHarmRes}
  Assume the conditions of Theorem~\ref{almostHarmSGD}. If
$G({\bf a, \boldsymbol{\theta}}) \leq G(\boldsymbol{0,0}) - \delta\sqrt{G(\boldsymbol{0,0})}$
  and $(\boldsymbol{a,\theta}) \in \mathcal{M}_{G,\poly(\delta,1/d)}$,
  then there exists some $i, j$ such that $\|\theta_i - w_j\| <\epsilon$.
\end{lemma}
 
 \begin{proof}
 If there does not exists $i, j$ such that
   $\|\theta_i - w_j\| <\epsilon$, then by Lemma \ref{eigConv}, this implies $a_i^2 < \delta^2/\poly(d)$ for all $i$ (for $\beta\lambda = \Omega(1)$). Now, for a integrable
   function $f(x)$, $\| f\|_X = \sqrt{\expt_X[f(X)^2]}$ is a
   norm. Therefore, if $f(x) = \sum_i b_i \sigma(w_i,x)$ be our true
   target function, we conclude that by triangle inequality
\begin{align*}
\sqrt{G(\boldsymbol{a,\theta})}  & \geq \norm{\sum_{i=1}^k a_i \sigma(\theta_i,x) - f(x)}_X \\
&\geq \|f(x)\|_X\ - \sum_{i=1}^k \|a_i\sigma(\theta_i,x) \|_X \\
& \geq
  \sqrt{G(\boldsymbol{0,0})} - \delta/2
\end{align*}
Squaring gives a contradiction, so we conclude that there must exist $i, j$ such that $\theta_i$ is in a $\epsilon$ neighborhood of $w_j$.
 \end{proof}
 
 \begin{lemma}[Initialization]\label{almostHarmInitialize}
Assume the conditions of Lemma~\ref{almostHarmConv}. If $G(\boldsymbol{0,0}) \geq \epsilon$, then we can initialize $\boldsymbol{(a^{(0)},\theta^{(0)})}$ such that $G({\bf a^{(0)}, \boldsymbol{\theta^{(0)}}}) \leq G(\boldsymbol{0,0}) - 2\delta\sqrt{G(\boldsymbol{0,0})}$ with $\delta = (d/\epsilon)^{-O(d)}$ with high probability.
 \end{lemma}
 
 \begin{proof}
  Consider choosing $\theta_1 = {\bf 0}$ and then
  optimizing $a_1$. Given $\theta_1$, the loss decrease is:
%
\begin{align*}
   G(a_1,{\bf 0}) - G({\bf 0},{\bf 0}) & = \min_{a_1} 2a_1^2 +
  2\sum_{j=1}^k a_1 b_j\Phi_\epsilon({\bf 0},w_j) \\
 %
 & = -\frac{1}{2}\left(  \sum_{j=1}^k b_j
   \Phi_\epsilon({\bf 0},w_j)\right)^2 
\end{align*}

Because $\|w_j\| \leq d$, we see that the unnormalized potential $\widetilde{\Phi}_\epsilon$ of Lemma~\ref{almostHarmReal} satisfies $\widetilde{\Phi}_\epsilon({\bf 0}, w_j) \geq e^{-O(d)}$. Furthermore, since $|\widetilde{\Phi}_\epsilon({\bf 0})| \leq  (d/\epsilon)^{O(d)}$, we conclude that our normalized potential $\Phi_\epsilon$ satisfies $\Phi_\epsilon({\bf 0}, w_j) \geq (d/\epsilon)^{-O(d)}$. And since $b_j$ are uniformly chosen in $[-1,1]$, we conclude that with high probability over the choices of $b_j$, $-\frac{1}{2}\left(  \sum_{j=1}^k b_j\Phi(\theta_1,w_j)\right)^2 \geq (d/\epsilon)^{O(d)}$ by appealing to Chebyshev's inequality for uniform variables.

Therefore, we conclude that with high probability, $G(a_1, {\bf 0}) \leq G(\boldsymbol{0,0}) - (d/\epsilon)^{O(d)}$. Since $G(\boldsymbol{0,0}) \geq \epsilon$, we derive our claim.


%%Cramer-Rao type Argument
\if{1}
Let $f(x) =  \sum_{j=1}^k b_j \Phi(x,w_j)$, then it suffices (***Justify later) to show that $\Var(f(X)) \geq 1/\poly(d) $, where $X$ is a multivariate standard normal. By appealing to Cramer-Rao type bounds given in \cite{cacoullos1982upper}, we see that

\begin{align*}
 \Var(f(X)) &\geq \frac{1}{d}\expt \left[\sum_{i=1}^d \frac{\partial}{\partial x_i} f(X)\right]^2 \\
 &= \frac{1}{d}\left( \sum_{j=1}^k b_j \sum_{i=1}^d \expt \frac{\partial}{\partial x_i}\Phi(X,w_j) \right)^2 \\
\end{align*}

By Stein's identity, $\expt[\frac{\partial}{\partial x_i}\Phi(X,w_j)] = \expt[X_i\Phi(X,w_j)]$. And since $\Phi$ is translationally symmetric, if we let $w_{ji}$ be the $i$-th coordinate of $w_j$, then $\expt[ X_i\Phi(X,w_j)] = \expt[(X_i - w_{ji})\Phi(X, {\bf 0})] = -w_{ji}\expt[\Phi(X,{\bf 0})] = -Cw_{ji}$, where $C$ is a positive constant and $C\geq 1/\poly(d)$ (justify later).

Therefore, $\Var(f(X)) \geq \frac{C^2}{d} \left(\sum_{j=1}^k b_j \sum_{i=1}^d w_{ji}\right)^2$. However, note that $w_{ji}$ are independent Gaussians of variance 1, so we conclude that $\sum_{j=1}^k b_j \sum_{i=1}^d w_{ji}$ is a Gaussian of variance $d\|{\bf b}\|^2 \geq d$. So, with high probability over $w_1,...,w_j$, we conclude that $\Var(f(X)) \geq C^2 \geq 1/\poly(d)$.
\fi
\end{proof}
%

\begin{proof}[Proof of Theorem \ref{almostHarmSGD}]
Lemma~\ref{almostHarmReal} ensures all derivatives of $\Phi$ are bounded by $(d/\epsilon)^{O(d)}$. Lemma~\ref{strongConvergeTwo} ensures that we are in $\mathcal{M}_{G,\poly(\delta,1/d)}$ in $\poly(1/\delta)(d/\epsilon)^{O(d)}$ iterations.
If $G(\boldsymbol{0,0}) \leq \epsilon$, then by Lemma~\ref{strongConvergeTwo}, with high probability, $G(\boldsymbol{a,\theta}) \leq G(\boldsymbol{0,0}) \leq \epsilon$. Otherwise, by Lemma \ref{almostHarmInitialize},  we can initialize $\boldsymbol{a^{(0)},\theta^{(0)}}$ such that $G(\boldsymbol{a^{(0)},\theta^{(0)}}) \leq  G(0,0) - 2\delta \sqrt{G(0,0)}$ for $\delta = (d/\epsilon)^{-O(d)}$. Finally, we conclude by Theorem \ref{almostHarmRes}.
\end{proof}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "icmlpaper2017.tex"
%%% End:
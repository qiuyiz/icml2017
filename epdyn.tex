\section{Deep Learning, Potentials, and Electron-Proton Dynamics}
\label{sec:epdyn}
\paragraph{Preliminaries.}
We will work in the space  $\mathcal{M}= \R^d.$ 
We denote the gradient and Hessian as $\nabla_{\R^d} f$ and  $\nabla_{\R^d}^2 f$ respectively.
The Laplacian is defined as
$\Delta_{\R^d} f = \Tr(\nabla_{\R^d}^2 f)$. 
%
If $f$ is multivariate with variable $x_i$, then let $f_{x_i}$ be a
restriction of $f$ onto the variable $x_i$ with all other variables
fixed. Let $\nabla_{x_i}f, \Delta_{x_i}f$ to be the gradient and
Laplacian, respectively, of $f_{x_i}$ with respect to $x_i$. Lastly,
we say $x$ is a critical point of $f$ if $\nabla f$ does not exist or
$\nabla f = 0$. 

We focus on learning depth two networks with a linear activation on
the output node. If the network takes inputs $x \in \R^d$ (say from
some distribution $\mathcal{D}$), then the network output, denoted
$f(x)$ is a sum over $k = \poly(d)$ hidden units with weight vectors
$w_{i} \in \R^d,$ activation $\sigma(x,w):\R^d \times \R^d\to \R,$ and
output weights $b_i \in \R.$ Thus, we can write
$f(x) = \sum_{i=1}^k b_i\sigma(x,w_i)$. We denote this concept class
$\mathcal{C}_{\sigma,k}.$ Our hypothesis concept class is also
$\mathcal{C}_{\sigma,k}.$ 

% of our learning
% procedure is the output of a two-layer neural network with linear
% output activation and has the form $f(x) = \sum_{i=1}^k
% b_i\sigma(x,w_i)$. 


%  where $\sigma(x,w):\R^d \times \R^d\to \R$
% is the activation function that takes in a hidden weight vector
% $w \in \R^d$ and the input vector $x \in \R^d$.  

% given by the form
% $f(x) = \sum_{i=1}^k b_i\sigma(x,w_i)$. 

 
% The target concept class $\mathcal{C}_{\sigma,k}$ of our learning
% procedure is the output of a two-layer neural network with linear
% output activation and has the form $f(x) = \sum_{i=1}^k
% b_i\sigma(x,w_i)$. 

Let $\boldsymbol{a} = (a_1,...,a_k)$ and $\boldsymbol{\theta} =
(\theta_1,...,\theta_k)$; similarly for $\boldsymbol{b},
\boldsymbol{w}$ and our guess is $\hat{f}(x) = \sum_{i=1}^k a_i
\sigma(x, \theta_i)$. 
We define $\Phi,$ the {\bf potential function} {\it corresponding to the activation
  } $\sigma,$ as
\[\Phi(\theta, w) = \expt_{X\sim D}[ \sigma(X,\theta) \sigma(X,w)].\]
%
We work directly with the true squared loss error
$L(a,\theta)= \expt_{x \sim \mathcal{D}}[(f - \hat{f})^2]$. To
simplify $L$, we re-parametrize $a$ by $-a$ and expand.
%
%\begin{equation}\label{errEmp}
%\widehat{L}(\boldsymbol{a,\theta})  = \frac{1}{n}\sum_{j=1}^n \left(\sum_{i=1}^k %b_i\sigma(x_j,w_i) - \sum_{i=1}^k a_i \sigma(x_j,\theta_i)\right)^2
%\end{equation}
%
\begin{align}
 L(\boldsymbol{a,\theta})  & = \expt_{X\sim D}\left[ \left(
  \sum_{i=1}^k a_i \sigma(X,\theta_i) + \sum_{i=1}^k
  b_i\sigma(X,w_i)\right)^2\right] \nonumber \\
%
& = \sum_{i=1}^k \sum_{j=1}^k a_i a_j \Phi(\theta_i,\theta_j)+ 2 a_ib_j \Phi(\theta_i,w_j)+ b_i b_j \Phi(w_i,w_j),
 \label{errLoss}
\end{align}
%
Given $\mathcal{D},$ the activation function $\sigma$, and the loss
$L$, we attempt to show that we can use some variant of gradient
descent to learn, with high probability, an $\epsilon$-approximation
of $w_j$ for some (or all) $j$. 
Note that our loss is jointly convex, though it is quadratic in
$\boldsymbol{a}$. 








% \subsection{Activation-Potential Correspondence}
In this paper, we restrict our attention to translationally invariant activations
and potentials.
%
% We restrict our attention to potential (and activation) functions with some natural symmetry, so they are either {\it translationally or rotationally invariant}.
Specifically, we may write $\Phi= h(\theta-w)$ for some function $h(x).$ Furthermore, a translationally invariant function $\Phi(r)$ is {\it radial} if it is a function of $r = \|x - y \|$.
%   in the first case, with $\theta, w \in \R^d$. In the second case, $\Phi = h(\theta^Tw)$ and we enforce $\theta, w \in S^{d-1}$. Such potentials are called {\bf symmetric}. Our results focus on rotationally invariant potentials, as they correspond to classical neural networks.

{\bf Remark}: Translationally symmetric potentials satisfy
$\Phi(\theta,\theta)$ {\it is a positive constant}. We normalize
$\Phi(\theta,\theta) = 1$ for the rest of the paper.
  
We assume that our input distribution
$\mathcal{D} = \mathcal{N}(0, {\bf I_{d\times d}})$ is fixed as the
standard Gaussian in $\R^d$. This assumption is not critical and a
simpler distribution might lead to better bounds. However, for
arbitrary distributions, there are hardness results for PAC-learning
halfspaces \cite{klivans2006cryptographic}.

We call a potential function {\bf realizable} if it corresponds to
some activation $\sigma$.  The following theorem characterizes
realizable translationally invariant potentials under standard
Gaussian inputs. Proofs and a similar characterization for rotationally invariant
potentials can be found in \supmaterial{Appendix~\ref{sec:realizable}}.
%  We briefly state some results about
% characterizations of realizable potentials for translationally and
% rotationally invariant potentials. Full proofs and calculations of
% activation-potential correspondences
% % , such as those claimed in Table \ref{table1}, 
% can be found in the supplementary material.
%
\begin{restatable}{theorem}{tranreal}
\label{thm:tranReal}
Let $\mathcal{M} = \R^d$ and $\Phi$ is square-integrable and
$\FT(\Phi)$ is integrable. Then, $\Phi$ is realizable under standard
Gaussian inputs if $\mathfrak{F}(\Phi)(\omega) \geq 0$ and the
corresponding activation is
$\sigma(x) =
(2\pi)^{d/4}e^{x^Tx/4}\mathfrak{F}^{-1}(\sqrt{\mathfrak{F}(\Phi)})(x),
$ where $\mathfrak{F}$ is the generalized Fourier transform in $\R^d.$
\end{restatable}
%

% \Anote{give examples of phi's and corresponding sigmas in in a paragraph.}
%
\subsection{Electron-Proton Dynamics}

%
% Our main observation is that the gradient descent dynamics of learning such to layer networks is equivalent to the dynamics of a set of proton-electron charges under a certain electrical attraction force function. 
% {\color{red} Assume for now that the coefficients $b_i$ and $a_i$ are $1$. 
% Thus we are only perform gradient descent over $\theta_i$ to minimize the expected square loss of $f-\widetilde{f}$.
%
% The charges reside in $R^d$. The protons are fixed at locations
% $w_1,..,w_k$. The electrons are at positions $\theta_1,..,\theta_k$
% and can move: 

By interpreting the pairwise potentials as electrostatic attraction
potentials, we notice that our dynamics is similar to electron-proton
type dynamics under potential $\Phi$, where $w_i$ are fixed point
charges in $\R^d$ and $\theta_i$ are moving point charges in $\R^d$
that are trying to find $w_i$. The total force on each charge is the
sum of the pairwise forces, determined by the gradient of $\Phi.$ We
note that standard dynamics interprets the force between particles as
an acceleration vector. In gradient descent, it is interpreted as a
velocity vector. 
%
\begin{definition}\label{EPDef}
  Given a potential $\Phi$ and particle locations
  $\theta_1,...,\theta_k \in \rea^d$ along with their respective
  charges $a_1,...,a_k \in \R$. We define {\bf Electron-Proton
    Dynamics} under $\Phi$ with some subset $S \subseteq [k]$ of fixed
  particles to be the solution $(\theta_1(t),...,\theta_k(t))$ to the
  following system of differential equations: For each pair
  $(\theta_i,\theta_j)$, there is a force from $\theta_j$ exerted on
  $\theta_i$ that is given by
  ${\bf F}_{i}(\theta_j) = a_ia_j\nabla_{\theta_i}
  \Phi(\theta_i,\theta_j)$ and
\[-\frac{d\theta_i}{dt} =  \sum_{ j \neq i} {\bf F}_{i}(\theta_j)\]
for all $i \not \in S$, with $\theta_i(0) = \theta_i$. For $i \in S$, $\theta_i(t) =  \theta_i$.
\end{definition}
%
For the following theorem, we assume that $\boldsymbol{\theta}$ is fixed.
%  ${\color{red} Although our loss is jointly
% non-convex, it is quadratic in Therefore we can
% restrict our attention to convergence in $\boldsymbol{\theta}$ to
% $\boldsymbol{w}$, since we have convergence guarantees to
% $\boldsymbol{a^*(\theta)}$, the optimal set of output weights of a
% given. }
\begin{restatable}{theorem}{epdyn}
\label{EPDyn}
Let $\Phi$ be a symmetric potential and $L$ be as in \eqref{errLoss}. Running continuous gradient descent on $\frac{1}{2} L$ with respect to $\theta$, initialized at
$(\theta_1,...,\theta_k)$ produces the same dynamics as
Electron-Proton Dynamics under $2\Phi$ with fixed particles at
$w_1,...,w_k$ with respective charges $b_1,..,b_k$ and moving
particles at $\theta_1,...,\theta_k$ with respective charges
$a_1,...,a_k$.
\end{restatable} 


%

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "icmlpaper2017.tex"
%%% End:
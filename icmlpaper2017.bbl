\begin{thebibliography}{10}

\bibitem{ChoromanskaHMAL14}
Anna Choromanska, Mikael Henaff, Micha{\"{e}}l Mathieu, G{\'{e}}rard~Ben Arous,
  and Yann LeCun.
\newblock The loss surface of multilayer networks.
\newblock {\em CoRR}, abs/1412.0233, 2014.

\bibitem{DauphinPGCGB14}
Yann Dauphin, Razvan Pascanu, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Kyunghyun Cho,
  Surya Ganguli, and Yoshua Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock {\em CoRR}, abs/1406.2572, 2014.

\bibitem{Kawaguchi16a}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock {\em CoRR}, abs/1605.07110, 2016.

\bibitem{valiant2014learning}
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li~Zhang.
\newblock Learning polynomials with neural networks.
\newblock 2014.

\bibitem{GeHJY15}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle points - online stochastic gradient for tensor
  decomposition.
\newblock {\em CoRR}, abs/1503.02101, 2015.

\bibitem{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock {\em arXiv preprint arXiv:1703.00887}, 2017.

\bibitem{ClevertUH15}
Djork{-}Arn{\'{e}} Clevert, Thomas Unterthiner, and Sepp Hochreiter.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock {\em CoRR}, abs/1511.07289, 2015.

\bibitem{ShahKSS16}
Anish Shah, Eashan Kadam, Hena Shah, and Sameer Shinde.
\newblock Deep residual networks with exponential linear unit.
\newblock {\em CoRR}, abs/1604.04112, 2016.

\bibitem{SoudryC16}
Daniel Soudry and Yair Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock {\em CoRR}, abs/1605.08361, 2016.

\bibitem{Auer}
Peter Auer, Mark Herbster, and Manfred~K Warmuth.
\newblock Exponentially many local minima for single neurons.
\newblock pages 316--322, 1996.

\bibitem{brady1989back}
Martin~L Brady, Raghu Raghavan, and Joseph Slawny.
\newblock Back propagation fails to separate where perceptrons succeed.
\newblock {\em IEEE Transactions on Circuits and Systems}, 36(5):665--674,
  1989.

\bibitem{BlumR88}
Avrim Blum and Ronald~L. Rivest.
\newblock Training a 3-node neural network is np-complete.
\newblock pages 9--18, 1988.

\bibitem{klivans2006cryptographic}
Adam~R Klivans and Alexander~A Sherstov.
\newblock Cryptographic hardness for learning intersections of halfspaces.
\newblock In {\em 2006 47th Annual IEEE Symposium on Foundations of Computer
  Science (FOCS'06)}, pages 553--562. IEEE, 2006.

\bibitem{LivniSS14}
Roi Livni, Shai Shalev{-}Shwartz, and Ohad Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock {\em CoRR}, abs/1410.1141, 2014.

\bibitem{ZhangLWJ15}
Yuchen Zhang, Jason~D. Lee, Martin~J. Wainwright, and Michael~I. Jordan.
\newblock Learning halfspaces and neural networks with random initialization.
\newblock {\em CoRR}, abs/1511.07948, 2015.

\bibitem{ZhangLJ15}
Yuchen Zhang, Jason~D. Lee, and Michael~I. Jordan.
\newblock l1-regularized neural networks are improperly learnable in polynomial
  time.
\newblock {\em CoRR}, abs/1510.03528, 2015.

\bibitem{JanzaminSA15}
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods.
\newblock {\em CoRR}, abs/1506.08473, 2015.

\bibitem{AroraBGM13}
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.
\newblock Provable bounds for learning some deep representations.
\newblock {\em CoRR}, abs/1310.6343, 2013.

\bibitem{shalev2011learning}
Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan.
\newblock Learning kernel-based halfspaces with the 0-1 loss.
\newblock {\em SIAM Journal on Computing}, 40(6):1623--1646, 2011.

\bibitem{GoelKKT16}
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler.
\newblock Reliably learning the relu in polynomial time.
\newblock {\em CoRR}, abs/1611.10258, 2016.

\bibitem{arnold1985mathematical}
Vladimir~I Arnold, Valery~V Kozlov, and Anatoly~I Neishtadt.
\newblock Mathematical aspects of classical and celestial mechanics.
\newblock {\em Encyclopaedia Math. Sci}, 3:1--291, 1985.

\bibitem{DanielyFS16}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock {\em CoRR}, abs/1602.05897, 2016.

\end{thebibliography}
